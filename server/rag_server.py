"""
RAG Server - éƒ¨ç½²åœ¨æœ‰ GPU çš„æœåŠ¡å™¨ä¸Š
æ”¯æŒ Web Searchã€RAG Search å’Œ Agentic Search

è¿è¡Œæ–¹å¼ï¼š
    python rag_server.py --port 8000 --host 0.0.0.0

ç¯å¢ƒå˜é‡é…ç½®ï¼š
    export OPENAI_API_KEY="your-key"
    export DASHSCOPE_API_KEY="your-key"  # å¦‚æœä½¿ç”¨ Qwen æ¨¡å‹
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
import argparse
import os
import time
from datetime import datetime

# å¦‚æœä½¿ç”¨ GPU åŠ è½½æ¨¡å‹
try:
    import torch
    from sentence_transformers import SentenceTransformer
    # from transformers import AutoTokenizer, AutoModel
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {DEVICE}")
except ImportError:
    DEVICE = "cpu"
    print("âš ï¸ PyTorch not found, using CPU mode")

app = FastAPI(
    title="LocalSearchBench RAG API",
    description="RAG Search API with GPU support",
    version="1.0.0"
)

# é…ç½® CORS - å…è®¸ Gradio å®¢æˆ·ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================

class RAGSearchRequest(BaseModel):
    query: str
    top_k: int = 5
    retriever: str = "qwen3-embedding-8b"
    reranker: str = "qwen3-reranker-8b"

class WebSearchRequest(BaseModel):
    query: str
    top_k: int = 10

class AgenticSearchRequest(BaseModel):
    query: str
    model: str = "gpt-4.1"
    max_iterations: int = 5

class SearchResult(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, float]
    reasoning_steps: Optional[List[str]] = None
    processing_time: float

# ==================== æ¨¡å‹åŠ è½½ï¼ˆGPUï¼‰====================

class RAGModels:
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹åˆ° GPU"""
    
    def __init__(self):
        self.embedding_model = None
        self.reranker_model = None
        self.llm = None
        
    def load_embedding_model(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """åŠ è½½ Embedding æ¨¡å‹åˆ° GPU"""
        if self.embedding_model is None:
            print(f"ğŸ“¥ Loading embedding model: {model_name}")
            # è¿™é‡Œä½¿ç”¨ sentence-transformers ä½œä¸ºç¤ºä¾‹
            # ä½ å¯ä»¥æ›¿æ¢ä¸º Qwen3-Embedding-8B æˆ–å…¶ä»–æ¨¡å‹
            try:
                self.embedding_model = SentenceTransformer(model_name, device=DEVICE)
                print(f"âœ… Embedding model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load embedding model: {e}")
                self.embedding_model = None
        return self.embedding_model
    
    def load_reranker_model(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """åŠ è½½ Reranker æ¨¡å‹åˆ° GPU"""
        if self.reranker_model is None:
            print(f"ğŸ“¥ Loading reranker model: {model_name}")
            try:
                from sentence_transformers import CrossEncoder
                self.reranker_model = CrossEncoder(model_name, device=DEVICE)
                print(f"âœ… Reranker model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load reranker model: {e}")
                self.reranker_model = None
        return self.reranker_model
    
    def encode_query(self, query: str):
        """ä½¿ç”¨ GPU è¿›è¡ŒæŸ¥è¯¢ç¼–ç """
        if self.embedding_model is None:
            self.load_embedding_model()
        
        if self.embedding_model:
            with torch.no_grad():
                embedding = self.embedding_model.encode(query, convert_to_tensor=True)
            return embedding.cpu().numpy()
        else:
            # Fallback: ä½¿ç”¨ç®€å•çš„æ–¹æ³•
            return None

# å…¨å±€æ¨¡å‹å®ä¾‹
models = RAGModels()

# ==================== RAG å®ç° ====================

def perform_rag_search(query: str, top_k: int, retriever: str, reranker: str) -> Dict:
    """
    çœŸå®çš„ RAG æœç´¢å®ç°
    
    æ›¿æ¢è¿™ä¸ªå‡½æ•°ä¸ºä½ çš„å®é™…å®ç°ï¼Œä¾‹å¦‚ï¼š
    - ä½¿ç”¨ Qwen3-Embedding-8B è¿›è¡Œæ£€ç´¢
    - ä½¿ç”¨ Qwen3-Reranker-8B è¿›è¡Œé‡æ’åº
    - è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    """
    start_time = time.time()
    
    # 1. ä½¿ç”¨ GPU è¿›è¡Œå‘é‡æ£€ç´¢
    query_embedding = models.encode_query(query)
    
    # 2. ä»å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆè¿™é‡Œéœ€è¦ä½ çš„å®ç°ï¼‰
    # ä¾‹å¦‚ï¼šä½¿ç”¨ FAISS, Milvus, Qdrant ç­‰
    retrieved_docs = [
        {
            "merchant_name": "æµ·åº•æç«é”…(é™†å®¶å˜´åº—)",
            "address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºé™†å®¶å˜´ä¸–çºªå¤§é“100å·",
            "rating": 4.8,
            "price": "äººå‡150å…ƒ",
            "description": "çŸ¥åç«é”…å“ç‰Œï¼ŒæœåŠ¡å¥½ï¼Œé£Ÿææ–°é²œ",
            "score": 0.92
        },
        {
            "merchant_name": "å°è¾‰å“¥ç«é”…(å—äº¬è¥¿è·¯åº—)", 
            "address": "ä¸Šæµ·å¸‚é™å®‰åŒºå—äº¬è¥¿è·¯1618å·",
            "rating": 4.6,
            "price": "äººå‡120å…ƒ",
            "description": "æ½®æ±•ç‰›è‚‰ç«é”…ï¼Œè‚‰è´¨é²œç¾",
            "score": 0.88
        }
    ]
    
    # 3. ä½¿ç”¨ GPU è¿›è¡Œé‡æ’åº
    if models.reranker_model:
        pairs = [[query, doc["description"]] for doc in retrieved_docs]
        rerank_scores = models.reranker_model.predict(pairs)
        for doc, score in zip(retrieved_docs, rerank_scores):
            doc["rerank_score"] = float(score)
        retrieved_docs = sorted(retrieved_docs, key=lambda x: x["rerank_score"], reverse=True)
    
    # 4. ç”Ÿæˆç­”æ¡ˆï¼ˆè°ƒç”¨ LLMï¼‰
    answer = f"æ ¹æ®æ‚¨çš„æŸ¥è¯¢ã€Œ{query}ã€ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹{len(retrieved_docs)}å®¶å•†æˆ·..."
    
    # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = {
        "precision": 0.85,
        "recall": 0.78,
        "ndcg": 0.82,
        "latency_ms": (time.time() - start_time) * 1000
    }
    
    return {
        "answer": answer,
        "sources": retrieved_docs[:top_k],
        "metrics": metrics,
        "processing_time": time.time() - start_time
    }

def perform_web_search(query: str, top_k: int) -> Dict:
    """ä¼ ç»Ÿ Web æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Web æœç´¢é€»è¾‘
    # ä¾‹å¦‚ï¼šElasticSearch, BM25 ç­‰
    
    results = [
        {
            "merchant_name": f"å•†æˆ· {i+1}",
            "address": f"ä¸Šæµ·å¸‚æŸåŒºæŸè¡—é“{i+1}å·",
            "rating": 4.5 - i * 0.1,
            "price": f"äººå‡{100 + i*20}å…ƒ"
        }
        for i in range(top_k)
    ]
    
    return {
        "answer": f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ",
        "sources": results,
        "metrics": {"latency_ms": (time.time() - start_time) * 1000},
        "processing_time": time.time() - start_time
    }

def perform_agentic_search(query: str, model: str, max_iterations: int) -> Dict:
    """æ™ºèƒ½ä½“æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Agent é€»è¾‘
    # ä¾‹å¦‚ï¼šä½¿ç”¨ LangChain, AutoGPT ç­‰
    
    reasoning_steps = [
        "ğŸ¤” åˆ†ææŸ¥è¯¢æ„å›¾...",
        "ğŸ” ç¬¬1æ­¥ï¼šæœç´¢ç›¸å…³å•†æˆ·...",
        "ğŸ“Š ç¬¬2æ­¥ï¼šè¿‡æ»¤å’Œæ’åºç»“æœ...",
        "ğŸ’¡ ç¬¬3æ­¥ï¼šç”Ÿæˆæ¨è..."
    ]
    
    results = [
        {
            "merchant_name": "æ¨èå•†æˆ· 1",
            "address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
            "rating": 4.8,
            "reason": "é«˜è¯„åˆ†ä¸”ç¬¦åˆæ‚¨çš„éœ€æ±‚"
        }
    ]
    
    return {
        "answer": "åŸºäºå¤šæ­¥æ¨ç†ï¼Œä¸ºæ‚¨æ¨è...",
        "sources": results,
        "metrics": {
            "correctness": 0.85,
            "completeness": 0.90,
            "faithfulness": 0.88,
            "latency_ms": (time.time() - start_time) * 1000
        },
        "reasoning_steps": reasoning_steps,
        "processing_time": time.time() - start_time
    }

# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
def root():
    return {
        "service": "LocalSearchBench RAG API",
        "version": "1.0.0",
        "device": DEVICE,
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "device": DEVICE,
        "gpu_available": torch.cuda.is_available() if 'torch' in globals() else False,
        "models_loaded": {
            "embedding": models.embedding_model is not None,
            "reranker": models.reranker_model is not None
        }
    }

@app.post("/api/rag/search", response_model=SearchResult)
async def rag_search(request: RAGSearchRequest):
    """RAG æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_rag_search(
            query=request.query,
            top_k=request.top_k,
            retriever=request.retriever,
            reranker=request.reranker
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/web/search", response_model=SearchResult)
async def web_search(request: WebSearchRequest):
    """Web æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_web_search(
            query=request.query,
            top_k=request.top_k
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/agentic/search", response_model=SearchResult)
async def agentic_search(request: AgenticSearchRequest):
    """Agentic æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_agentic_search(
            query=request.query,
            model=request.model,
            max_iterations=request.max_iterations
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹"""
    print("ğŸš€ Starting LocalSearchBench RAG Server...")
    print(f"ğŸ“ Device: {DEVICE}")
    
    # é¢„åŠ è½½æ¨¡å‹åˆ° GPU
    if DEVICE == "cuda":
        print("ğŸ“¥ Pre-loading models to GPU...")
        models.load_embedding_model()
        models.load_reranker_model()
        print("âœ… Models loaded successfully")
    else:
        print("âš ï¸ Running in CPU mode")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ‘‹ Shutting down LocalSearchBench RAG Server...")
    # æ¸…ç† GPU æ˜¾å­˜
    if DEVICE == "cuda" and 'torch' in globals():
        torch.cuda.empty_cache()

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="LocalSearchBench RAG Server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    args = parser.parse_args()
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     LocalSearchBench RAG Server                           â•‘
â•‘     Device: {DEVICE:48s} â•‘
â•‘     Host: {args.host:50s} â•‘
â•‘     Port: {args.port:50d} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(
        "rag_server:app",
        host=args.host,
        port=args.port,
        reload=args.reload,
        workers=args.workers,
        log_level="info"
    )

if __name__ == "__main__":
    main()

