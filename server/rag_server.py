"""
RAG Server - éƒ¨ç½²åœ¨æœ‰ GPU çš„æœåŠ¡å™¨ä¸Š
æ”¯æŒ Web Searchã€RAG Search å’Œ Agentic Search

è¿è¡Œæ–¹å¼ï¼š
    python rag_server.py --port 8000 --host 0.0.0.0 --config /path/to/config.yaml

ç¯å¢ƒå˜é‡é…ç½®ï¼š
    export OPENAI_API_KEY="your-key"
    export DASHSCOPE_API_KEY="your-key"  # å¦‚æœä½¿ç”¨ Qwen æ¨¡å‹
    export TUANSOU_CONFIG="/path/to/config.yaml"  # LLM é…ç½®æ–‡ä»¶è·¯å¾„

æ£€ç´¢ä¸é‡æ’ç­–ç•¥ï¼š
    æœ¬æœåŠ¡å™¨ä¸ interactive_merchant_search_vllm.py ä¿æŒé«˜åº¦ä¸€è‡´ï¼š
    - å€™é€‰æ–‡æ¡£å€æ•°ï¼šcandidate_multiplier = 5
    - ç›¸ä¼¼åº¦è®¡ç®—ï¼š(max_distance - distance) / max_distance
    - é‡æ’åºæ–‡æœ¬æ ¼å¼ï¼šname - category/subcategory - address + åœ°ç†ä½ç½®ï¼ˆå¿…é¡»ï¼‰+ å¤šä¸ªå¯é€‰å­—æ®µ
    - åœ°ç†ä½ç½®å­—æ®µï¼ˆå¿…é¡»å‚ä¸é‡æ’ï¼‰ï¼šcity, district, business_area, landmark
    - subcategory å­—æ®µï¼šå¦‚æœå­˜åœ¨ï¼Œä¼šæ‹¼æ¥åˆ° category åé¢ï¼ˆæ ¼å¼ï¼šcategory/subcategoryï¼‰

LLM ç²¾æ’ï¼ˆæ–°å¢ï¼‰ï¼š
    - åœ¨ rerank åï¼Œä½¿ç”¨ LLM ä» 20 ä¸ªå€™é€‰ä¸­é€‰å‡ºæœ€ç»ˆçš„ 5 ä¸ªç»“æœ
    - å¯é€šè¿‡è¯·æ±‚å‚æ•° use_llm_ranking æ§åˆ¶æ˜¯å¦å¯ç”¨ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    - LLM ä¼šç»¼åˆè€ƒè™‘ç”¨æˆ·æŸ¥è¯¢æ„å›¾ã€å•†æˆ·ä¿¡æ¯å®Œæ•´æ€§ã€è¯„åˆ†ç­‰å› ç´ 
    - é…ç½®æ–‡ä»¶éœ€åŒ…å« LLM API keys å’Œç›¸å…³é…ç½®ï¼ˆå‚è€ƒ auto_rag_merchant_search.pyï¼‰
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any, Tuple
import uvicorn
import argparse
import os
import time
from datetime import datetime
from pathlib import Path
import threading

# åŸºç¡€ä¾èµ–
import json
import yaml
import requests
import numpy as np
import aiohttp
import asyncio

# å¦‚æœä½¿ç”¨ GPU åŠ è½½æ¨¡å‹
try:
    import torch
    from sentence_transformers import SentenceTransformer, CrossEncoder
    import faiss
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {DEVICE}")
    HAS_GPU = torch.cuda.is_available()
except ImportError as e:
    DEVICE = "cpu"
    HAS_GPU = False
    print(f"âš ï¸ PyTorch/FAISS not found: {e}, using CPU mode")

app = FastAPI(
    title="LocalSearchBench RAG API",
    description="RAG Search API with GPU support",
    version="1.0.0"
)

# é…ç½® CORS - å…è®¸ Gradio å®¢æˆ·ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================

class RAGSearchRequest(BaseModel):
    query: str
    city: str = "ä¸Šæµ·"  # æ”¯æŒçš„åŸå¸‚ï¼ˆä¸­æ–‡ï¼‰
    top_k: int = 5  # æœ€ç»ˆè¿”å›5ä¸ªç»“æœ
    retriever: str = "qwen3-embedding-8b"  # é»˜è®¤ä½¿ç”¨ Qwen3-Embedding-8B
    reranker: str = "qwen3-reranker-8b"    # é»˜è®¤ä½¿ç”¨ Qwen3-Reranker-8B
    use_llm_ranking: bool = True  # æ˜¯å¦å¯ç”¨ LLM ç²¾æ’ï¼ˆé»˜è®¤å¯ç”¨ï¼‰

class WebSearchRequest(BaseModel):
    query: str
    top_k: int = 5

class AgenticSearchRequest(BaseModel):
    query: str
    model: str = "gpt-4.1"
    max_iterations: int = 5

class SearchResult(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, Any]  # æ”¹ä¸º Any ä»¥æ”¯æŒæ··åˆç±»å‹ï¼ˆfloatã€intã€strï¼‰
    reasoning_steps: Optional[List[str]] = None
    processing_time: float

# ==================== åŸå¸‚å‘é‡æ•°æ®åº“åŠ è½½å™¨ ====================

class CityVectorDB:
    """ç®¡ç†æ‰€æœ‰åŸå¸‚çš„FAISSå‘é‡æ•°æ®åº“ï¼ˆ1028ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, data_dir: str, use_gpu: bool = True):
        self.data_dir = data_dir
        self.use_gpu = use_gpu and torch.cuda.is_available()
        # åŸå¸‚æ˜ å°„ï¼šä¸­æ–‡ -> è‹±æ–‡ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        self.city_to_en = {
            "ä¸Šæµ·": "shanghai",
            "åŒ—äº¬": "beijing",
            "å¹¿å·": "guangzhou",
            "æ·±åœ³": "shenzhen",
            "æ­å·": "hangzhou",
            "è‹å·": "suzhou",
            "æˆéƒ½": "chengdu",
            "é‡åº†": "chongqing",
            "æ­¦æ±‰": "wuhan"
        }
        self.indexes = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.metadata = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.gpu_resources = None
        
        # åˆå§‹åŒ– GPU èµ„æº
        # æ³¨æ„ï¼šGPU å…¼å®¹æ€§æ£€æŸ¥åº”è¯¥åœ¨å¯åŠ¨è„šæœ¬ä¸­å®Œæˆï¼ˆstart_rag_server.shï¼‰
        # å› ä¸º FAISS çš„ C++ æ–­è¨€å¤±è´¥ä¼šå¯¼è‡´è¿›ç¨‹å´©æºƒï¼ŒPython æ— æ³•æ•è·
        if self.use_gpu:
            try:
                self.gpu_resources = faiss.StandardGpuResources()
                print(f"ğŸš€ GPU resources initialized for FAISS")
            except Exception as e:
                print(f"âš ï¸  Failed to initialize GPU resources: {e}")
                print(f"âš ï¸  Falling back to CPU mode")
                self.use_gpu = False
                self.gpu_resources = None
        
        self.load_all_cities()
    
    def load_all_cities(self):
        """åŠ è½½æ‰€æœ‰åŸå¸‚çš„å‘é‡æ•°æ®åº“"""
        device_info = "GPU" if self.use_gpu else "CPU"
        print(f"\nğŸ“¦ Loading vector databases from: {self.data_dir}")
        print(f"ğŸ’» Device: {device_info}")
        
        for city_cn, city_en in self.city_to_en.items():
            try:
                # åŠ è½½ 1028 ç‰ˆæœ¬çš„æ•°æ®ï¼ˆæ–‡ä»¶åä½¿ç”¨è‹±æ–‡ï¼‰
                index_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028.faiss")
                meta_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028_metadata.json")
                
                if not os.path.exists(index_path) or not os.path.exists(meta_path):
                    print(f"âš ï¸  {city_cn}: Files not found")
                    continue
                
                # åŠ è½½ FAISS ç´¢å¼• (å…ˆåŠ è½½åˆ°CPU)
                cpu_index = faiss.read_index(index_path)
                
                # å¦‚æœå¯ç”¨GPUï¼Œå°†ç´¢å¼•è½¬ç§»åˆ°GPU
                if self.use_gpu:
                    try:
                        # å°†CPUç´¢å¼•è½¬æ¢ä¸ºGPUç´¢å¼•ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                        self.indexes[city_cn] = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
                        device_tag = "ğŸš€ GPU"
                    except Exception as e:
                        print(f"âš ï¸  {city_cn}: GPU transfer failed ({e}), using CPU")
                        self.indexes[city_cn] = cpu_index
                        device_tag = "ğŸ’» CPU"
                else:
                    self.indexes[city_cn] = cpu_index
                    device_tag = "ğŸ’» CPU"
                
                # åŠ è½½å…ƒæ•°æ®ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                with open(meta_path, "r", encoding="utf-8") as f:
                    self.metadata[city_cn] = json.load(f)
                
                print(f"âœ… {city_cn}: {self.indexes[city_cn].ntotal} vectors, {len(self.metadata[city_cn])} merchants [{device_tag}]")
            except Exception as e:
                print(f"âŒ Failed to load {city_cn}: {e}")
        
        print(f"\nğŸ‰ Loaded {len(self.indexes)}/{len(self.city_to_en)} cities successfully on {device_info}!\n")
    
    def search(self, query_embedding: np.ndarray, city: str = "ä¸Šæµ·", top_k: int = 20):
        """åœ¨æŒ‡å®šåŸå¸‚çš„å‘é‡æ•°æ®åº“ä¸­æœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            city: åŸå¸‚åï¼ˆä¸­æ–‡ï¼‰ï¼Œå¦‚ "ä¸Šæµ·"ã€"åŒ—äº¬"
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        if city not in self.indexes:
            raise ValueError(f"City '{city}' not loaded. Available cities: {list(self.indexes.keys())}")
        
        # ä½¿ç”¨ FAISS è¿›è¡Œå‘é‡æ£€ç´¢
        query_vec = query_embedding.reshape(1, -1).astype('float32')
        distances, indices = self.indexes[city].search(query_vec, top_k)
        
        # è·å–å¯¹åº”çš„å…ƒæ•°æ®
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.metadata[city]):
                merchant = self.metadata[city][idx].copy()
                merchant["vector_score"] = float(dist)
                results.append(merchant)
        
        return results

# ==================== LLM ç²¾æ’å™¨ ====================

class LLMRanker:
    """LLM ç²¾æ’å™¨ï¼šä» rerank çš„å€™é€‰ä¸­ç­›é€‰å‡ºæœ€ç»ˆç»“æœ"""
    
    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.llm = self._init_llm_config()
        self._api_keys: List[str] = self.llm.get("api_keys", [])
        self._key_index = 0
        # å»¶è¿Ÿåˆå§‹åŒ–é”ï¼Œé¿å…åœ¨äº‹ä»¶å¾ªç¯å¤–åˆ›å»º
        self._key_lock = None
        
    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        candidates: List[str] = []
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„è·¯å¾„
        if config_path:
            candidates.append(os.path.abspath(os.path.expanduser(config_path)))
        
        # å°è¯•ç¯å¢ƒå˜é‡
        env_cfg = os.getenv("TUANSOU_CONFIG") or os.getenv("CONFIG_PATH")
        if env_cfg:
            candidates.append(os.path.abspath(os.path.expanduser(env_cfg)))
        
        # ç¡¬ç¼–ç çš„æœåŠ¡å™¨é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆçº§é«˜ï¼‰
        candidates.append("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtsearch-assistant/ai-search/hehang03/config/config.yaml")
        
        # å°è¯•ç›¸å¯¹è·¯å¾„ï¼ˆMac æœ¬åœ°å¼€å‘ï¼‰
        candidates.append("config/config.yaml")
        candidates.append("../config/config.yaml")
        
        for cfg_path in candidates:
            if os.path.exists(cfg_path):
                try:
                    with open(cfg_path, "r", encoding="utf-8") as f:
                        return yaml.safe_load(f)
                except Exception as e:
                    print(f"âš ï¸ Failed to load config from {cfg_path}: {e}")
        
        # å¦‚æœæ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ï¼Œè¿”å›é»˜è®¤é…ç½®
        print("âš ï¸ No config file found, using default LLM config")
        return {}
    
    def _init_llm_config(self) -> Dict[str, Any]:
        """åˆå§‹åŒ– LLM é…ç½®"""
        llm_config = self.config.get("llm", {})
        defaults = {
            "provider": "openai",
            "model": "deepseek-v31-meituan",
            "base_url": "https://aigc.sankuai.com/v1/openai/native",
            "timeout": 300,
            "max_retries": 3,
            "temperature": 0.2,
        }
        for k, v in defaults.items():
            llm_config.setdefault(k, v)
        
        # è·å– API Keys
        api_keys = llm_config.get("api_keys") or []
        if not api_keys:
            env_key = os.getenv("OPENAI_API_KEY")
            if env_key:
                api_keys = [env_key]
        
        if not api_keys:
            llm_config["enabled"] = False
            print("âš ï¸ No API keys found, LLM ranking disabled")
        else:
            llm_config["api_keys"] = api_keys
            llm_config["enabled"] = True
            print(f"âœ… LLM ranking enabled with {len(api_keys)} API key(s)")
        
        return llm_config
    
    def _next_key(self) -> Optional[str]:
        """è½®è¯¢è·å–ä¸‹ä¸€ä¸ª API Key"""
        if not self._api_keys:
            return None
        key = self._api_keys[self._key_index % len(self._api_keys)]
        self._key_index += 1
        return key
    
    async def select_top_k_async(
        self, 
        query: str, 
        candidates: List[Dict[str, Any]], 
        top_k: int = 5,
        city: str = "ä¸Šæµ·"
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ LLM ä»å€™é€‰ä¸­ç­›é€‰å‡º top_k ä¸ªæœ€ç›¸å…³çš„å•†æˆ·
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            candidates: å€™é€‰å•†æˆ·åˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ rerank åçš„ç»“æœï¼‰
            top_k: è¿”å›ç»“æœæ•°é‡
            city: åŸå¸‚åç§°
            
        Returns:
            ç­›é€‰åçš„å•†æˆ·åˆ—è¡¨
        """
        if not self.llm.get("enabled", False):
            print("âš ï¸ LLM ranking disabled, returning top_k candidates as-is")
            return candidates[:top_k]
        
        if len(candidates) <= top_k:
            print(f"ğŸ“‹ Candidates count ({len(candidates)}) <= top_k ({top_k}), no LLM ranking needed")
            return candidates
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_selection_prompt(query, candidates[:20], top_k, city)
            
            # è°ƒç”¨ LLM
            content = await self._call_llm_async(prompt, temperature=0.0, max_tokens=8192)
            
            # è§£æç»“æœ
            selected_indices = self._parse_selection_result(content, len(candidates), top_k)
            
            # æ ¹æ®ç´¢å¼•è¿”å›ç»“æœ
            result = []
            for idx in selected_indices:
                if 0 <= idx < len(candidates):
                    merchant = candidates[idx].copy()
                    merchant['llm_selected'] = True
                    merchant['llm_rank'] = len(result) + 1
                    result.append(merchant)
            
            # å¦‚æœ LLM æˆåŠŸè§£æä½†é€‰æ‹©äº†è¾ƒå°‘çš„å•†æˆ·ï¼ˆåŒ…æ‹¬0ä¸ªï¼‰ï¼Œå°Šé‡è¿™ä¸ªåˆ¤æ–­
            if len(selected_indices) > 0:
                # LLM æˆåŠŸè¿”å›äº†é€‰æ‹©ï¼ˆå³ä½¿å°‘äº top_kï¼‰
                print(f"âœ… LLM selected {len(result)} merchants from {len(candidates)} candidates (requested: {top_k})")
                return result if result else candidates[:min(1, len(candidates))]  # è‡³å°‘è¿”å›1ä¸ªï¼Œé¿å…å®Œå…¨ä¸ºç©º
            else:
                # LLM è¿”å›ç©ºåˆ—è¡¨ï¼Œè¯´æ˜æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„ï¼Œä½†ä¸ºäº†ä¿è¯ç”¨æˆ·ä½“éªŒï¼Œè¿”å›top 1
                print(f"âš ï¸ LLM returned empty selection, returning top 1 candidate")
                return candidates[:min(1, len(candidates))]
                
        except Exception as e:
            print(f"âŒ LLM ranking error: {e}, falling back to top_k")
            return candidates[:top_k]
    
    def _build_selection_prompt(
        self, 
        query: str, 
        candidates: List[Dict[str, Any]], 
        top_k: int,
        city: str
    ) -> str:
        """æ„å»º LLM ç­›é€‰æç¤ºè¯"""
        # æ ¼å¼åŒ–å€™é€‰å•†æˆ·ä¿¡æ¯
        formatted_candidates = []
        for i, doc in enumerate(candidates, 0):
            name = doc.get('name', 'æœªçŸ¥')
            category = doc.get('category', '')
            subcategory = doc.get('subcategory', '')
            address = doc.get('address', '')
            rating = doc.get('rating', '')
            price = doc.get('price_range', '')
            district = doc.get('district', '')
            business_area = doc.get('business_area', '')
            tags = doc.get('tags', [])
            products = doc.get('products', '')
            hours = doc.get('business_hours', '')
            rerank_score = doc.get('rerank_score', 0)
            
            tags_str = ','.join(tags[:5]) if isinstance(tags, list) else str(tags)
            cat_str = f"{category}/{subcategory}" if subcategory else category
            
            formatted_candidates.append(
                f"{i}. åç§°ï¼š{name} | ç±»åˆ«ï¼š{cat_str} | åœ°å€ï¼š{address} | "
                f"åŒºåŸŸï¼š{district} {business_area} | è¯„åˆ†ï¼š{rating} | ä»·æ ¼ï¼š{price} | "
                f"æ ‡ç­¾ï¼š{tags_str} | æœåŠ¡ï¼š{products} | è¥ä¸šï¼š{hours} | é‡æ’åˆ†ï¼š{rerank_score:.4f}"
            )
        
        candidates_text = '\n'.join(formatted_candidates)
        
        prompt = f"""ä»»åŠ¡ï¼šä»ä¸‹æ–¹å€™é€‰å•†æˆ·ä¸­ï¼Œç­›é€‰å‡ºçœŸæ­£ç¬¦åˆç”¨æˆ·æŸ¥è¯¢éœ€æ±‚çš„å•†æˆ·ï¼ˆæœ€å¤š {top_k} ä¸ªï¼‰ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
åŸå¸‚ï¼š{city}

å€™é€‰å•†æˆ·ï¼ˆå…± {len(candidates)} ä¸ªï¼‰ï¼š
{candidates_text}

ç­›é€‰è¦æ±‚ï¼š
1. **ä¸¥æ ¼åŒ¹é…**ç”¨æˆ·æŸ¥è¯¢ä¸­çš„å…³é”®æ¡ä»¶ï¼ˆå¦‚åœ°ç‚¹ã€ä»·æ ¼ã€ç±»å‹ã€ç‰¹æ®Šéœ€æ±‚ç­‰ï¼‰
2. **åªé€‰æ‹©çœŸæ­£ç¬¦åˆæ¡ä»¶çš„å•†æˆ·**ï¼Œä¸è¦ä¸ºäº†å‡‘æ•°è€Œé€‰æ‹©ä¸å¤ªç›¸å…³çš„
3. ä¼˜å…ˆé€‰æ‹©è¯„åˆ†é«˜ã€ä¿¡æ¯å®Œæ•´ã€ç›¸å…³åº¦é«˜çš„å•†æˆ·
4. è€ƒè™‘é‡æ’åˆ†æ•°ï¼ˆrerank_scoreï¼‰ä½œä¸ºå‚è€ƒï¼Œä½†æœ€ç»ˆä»¥ç”¨æˆ·éœ€æ±‚ä¸ºå‡†
5. å¦‚æœç”¨æˆ·æŸ¥è¯¢ä¸­æåˆ°å…·ä½“åŒºåŸŸ/å•†åœˆï¼Œä¼˜å…ˆé€‰æ‹©è¯¥åŒºåŸŸçš„å•†æˆ·
6. ç¡®ä¿é€‰å‡ºçš„å•†æˆ·ä¿¡æ¯å……åˆ†ã€ä¸é‡å¤

æ•°é‡è¦æ±‚ï¼š
- æœ€å¤šé€‰æ‹© {top_k} ä¸ªå•†æˆ·
- å¦‚æœåªæœ‰ 2 å®¶çœŸæ­£ç¬¦åˆæ¡ä»¶ï¼Œå°±åªè¿”å› 2 å®¶ï¼Œä¸è¦å‡‘æ•°
- å¦‚æœæ²¡æœ‰å®Œå…¨ç¬¦åˆæ¡ä»¶çš„ï¼Œå¯ä»¥è¿”å›ç©ºåˆ—è¡¨

è¾“å‡ºæ ¼å¼ï¼š
ä»…è¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«å­—æ®µ "selected_indices"ï¼Œå€¼ä¸ºé€‰ä¸­çš„å•†æˆ·ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ã€‚
ä¾‹å¦‚ï¼š
- 5å®¶ç¬¦åˆï¼š{{"selected_indices": [0, 3, 5, 8, 12]}}
- 2å®¶ç¬¦åˆï¼š{{"selected_indices": [0, 5]}}
- 0å®¶ç¬¦åˆï¼š{{"selected_indices": []}}

æ³¨æ„ï¼š
- åªè¾“å‡º JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—
- selected_indices å¿…é¡»æ˜¯æ•´æ•°åˆ—è¡¨
- ç´¢å¼•èŒƒå›´ï¼š0 åˆ° {len(candidates)-1}
- å®ç¼ºæ¯‹æ»¥ï¼Œè´¨é‡ä¼˜å…ˆäºæ•°é‡
"""
        
        return prompt
    
    def _parse_selection_result(
        self, 
        content: str, 
        max_index: int, 
        top_k: int
    ) -> List[int]:
        """è§£æ LLM è¿”å›çš„é€‰æ‹©ç»“æœ"""
        try:
            # å°è¯•ç›´æ¥è§£æ JSON
            data = json.loads(content)
            if isinstance(data, dict) and "selected_indices" in data:
                indices = data["selected_indices"]
                if isinstance(indices, list):
                    # éªŒè¯å¹¶è¿‡æ»¤ç´¢å¼•
                    valid_indices = []
                    for idx in indices:
                        if isinstance(idx, int) and 0 <= idx < max_index:
                            if idx not in valid_indices:  # å»é‡
                                valid_indices.append(idx)
                    return valid_indices[:top_k]
        except json.JSONDecodeError:
            pass
        
        # å¦‚æœ JSON è§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
        import re
        numbers = re.findall(r'\b(\d+)\b', content)
        valid_indices = []
        for num_str in numbers:
            try:
                idx = int(num_str)
                if 0 <= idx < max_index and idx not in valid_indices:
                    valid_indices.append(idx)
                    if len(valid_indices) >= top_k:
                        break
            except ValueError:
                continue
        
        if valid_indices:
            return valid_indices
        
        # å¦‚æœå®Œå…¨å¤±è´¥ï¼Œè¿”å›å‰ top_k ä¸ªç´¢å¼•
        return list(range(min(top_k, max_index)))
    
    async def _call_llm_async(
        self, 
        prompt: str, 
        temperature: float, 
        max_tokens: int
    ) -> str:
        """å¼‚æ­¥è°ƒç”¨ LLM"""
        if not self.llm.get("enabled", False):
            return '{"selected_indices": []}'
        
        url = f"{self.llm['base_url']}/chat/completions"
        retries = max(1, int(self.llm.get("max_retries", 3)))
        timeout = int(self.llm.get("timeout", 300))
        
        last_err = None
        
        for attempt in range(retries):
            # è·å– API Key
            if self._key_lock is None:
                try:
                    loop = asyncio.get_running_loop()
                    self._key_lock = asyncio.Lock()
                except RuntimeError:
                    pass
            
            if self._key_lock:
                async with self._key_lock:
                    api_key = self._next_key()
            else:
                api_key = self._next_key()
            
            if not api_key:
                return '{"selected_indices": []}'
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}",
            }
            body = {
                "model": self.llm["model"],
                "messages": [{"role": "user", "content": prompt}],
                "temperature": temperature,
                "max_tokens": max_tokens,
            }
            
            try:
                timeout_cfg = aiohttp.ClientTimeout(total=timeout)
                start_ts = time.time()
                print(f"[LLM] Attempt {attempt+1}/{retries}, model={self.llm['model']}, prompt_len={len(prompt)}")
                
                async with aiohttp.ClientSession(timeout=timeout_cfg) as session:
                    async with session.post(url, headers=headers, json=body) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            content = data["choices"][0]["message"]["content"].strip()
                            latency = (time.time() - start_ts) * 1000.0
                            print(f"[LLM] Success: {resp.status}, latency={latency:.0f}ms")
                            return content
                        else:
                            txt = await resp.text()
                            last_err = f"{resp.status} {txt[:200]}"
                            latency = (time.time() - start_ts) * 1000.0
                            print(f"[LLM] Error: {resp.status}, latency={latency:.0f}ms, detail={last_err[:100]}")
                            
                            # 429 æ—¶é€€é¿
                            if resp.status == 429 and attempt < retries - 1:
                                backoff = 2 ** attempt
                                await asyncio.sleep(backoff)
                                continue
            except Exception as e:
                last_err = str(e)
                latency = (time.time() - start_ts) * 1000.0
                print(f"[LLM] Exception: latency={latency:.0f}ms, error={last_err[:100]}")
            
            # ç®€å•é€€é¿
            if attempt < retries - 1:
                await asyncio.sleep(1.5 * (attempt + 1))
        
        raise Exception(f"LLM è°ƒç”¨å¤±è´¥: {last_err}")

# ==================== æ¨¡å‹åŠ è½½ï¼ˆGPUï¼‰====================

class RAGModels:
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹åˆ° GPU"""
    
    def __init__(self, data_dir: str = None, use_gpu: bool = True, config_path: str = None):
        self.embedding_model = None
        self.reranker_model = None
        self.llm = None
        self.vector_db = None
        self.llm_ranker = None
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒ GPU åŠ é€Ÿï¼‰
        if data_dir and os.path.exists(data_dir):
            try:
                self.vector_db = CityVectorDB(data_dir, use_gpu=use_gpu)
            except Exception as e:
                print(f"âš ï¸ Failed to load vector databases: {e}")
        
        # åˆå§‹åŒ– LLM ç²¾æ’å™¨
        try:
            self.llm_ranker = LLMRanker(config_path=config_path)
        except Exception as e:
            print(f"âš ï¸ Failed to initialize LLM ranker: {e}")
        
    def load_embedding_model(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """åŠ è½½ Embedding æ¨¡å‹åˆ° GPU"""
        if self.embedding_model is None:
            print(f"ğŸ“¥ Loading embedding model: {model_name}")
            # è¿™é‡Œä½¿ç”¨ sentence-transformers ä½œä¸ºç¤ºä¾‹
            # ä½ å¯ä»¥æ›¿æ¢ä¸º Qwen3-Embedding-8B æˆ–å…¶ä»–æ¨¡å‹
            try:
                self.embedding_model = SentenceTransformer(model_name, device=DEVICE)
                print(f"âœ… Embedding model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load embedding model: {e}")
                self.embedding_model = None
        return self.embedding_model
    
    def load_reranker_model(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """åŠ è½½ Reranker æ¨¡å‹åˆ° GPU"""
        if self.reranker_model is None:
            print(f"ğŸ“¥ Loading reranker model: {model_name}")
            try:
                from sentence_transformers import CrossEncoder
                self.reranker_model = CrossEncoder(model_name, device=DEVICE)
                print(f"âœ… Reranker model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load reranker model: {e}")
                self.reranker_model = None
        return self.reranker_model
    
    def encode_query(self, query: str):
        """ä½¿ç”¨ GPU è¿›è¡ŒæŸ¥è¯¢ç¼–ç """
        if self.embedding_model is None:
            self.load_embedding_model()
        
        if self.embedding_model:
            with torch.no_grad():
                embedding = self.embedding_model.encode(query, convert_to_tensor=True)
            return embedding.cpu().numpy()
        else:
            # Fallback: ä½¿ç”¨ç®€å•çš„æ–¹æ³•
            return None

# å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆç¨ååœ¨ startup æ—¶åˆå§‹åŒ–ï¼‰
models = None

# ==================== RAG å®ç° ====================

async def perform_rag_search(query: str, city: str, top_k: int, retriever: str, reranker: str, use_llm_ranking: bool = True) -> Dict:
    """
    çœŸå®çš„ RAG æœç´¢å®ç°ï¼ˆä½¿ç”¨1028ç‰ˆæœ¬å‘é‡æ•°æ®åº“ï¼‰
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
    2. åœ¨æŒ‡å®šåŸå¸‚çš„ FAISS ç´¢å¼•ä¸­æ£€ç´¢ï¼ˆå€™é€‰æ–‡æ¡£æ•°é‡ = top_k Ã— candidate_multiplierï¼‰
    3. ä½¿ç”¨ Reranker æ¨¡å‹é‡æ’åº
    4. è¿”å› top_k ç»“æœ
    
    å‚è€ƒç­–ç•¥ï¼ˆä¸ interactive_merchant_search_vllm.py ä¿æŒä¸€è‡´ï¼‰ï¼š
    - å€™é€‰æ–‡æ¡£å€æ•°ï¼š5å€ï¼ˆå³æ£€ç´¢ top_k Ã— 5 ä¸ªå€™é€‰æ–‡æ¡£ï¼‰
    - ç›¸ä¼¼åº¦è½¬æ¢ï¼šå°† L2 è·ç¦»è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦åˆ†æ•°
    - é‡æ’åºæ–‡æœ¬ï¼šæ„å»ºåŒ…å«åœ°ç†ä½ç½®ï¼ˆcity/district/business_area/landmarkï¼‰+ å¤šä¸ªå…³é”®å­—æ®µçš„ä¸°å¯Œæ–‡æœ¬è¡¨ç¤º
    - ä¿ç•™æ’åä¿¡æ¯ï¼šè®°å½•åŸå§‹æ’åã€é‡æ’åºåˆ†æ•°å’Œæœ€ç»ˆæ’å
    """
    start_time = time.time()
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åŠ è½½
    if not models.vector_db:
        raise HTTPException(status_code=503, detail="Vector database not loaded. Please check server configuration.")
    
    if city not in models.vector_db.indexes:
        available_cities = list(models.vector_db.indexes.keys())
        raise HTTPException(
            status_code=400, 
            detail=f"City '{city}' not available. Available cities: {available_cities}"
        )
    
    try:
        # 1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
        embedding_start = time.time()
        query_embedding = models.encode_query(query)
        if query_embedding is None:
            raise HTTPException(status_code=503, detail="Embedding model not loaded")
        embedding_time = time.time() - embedding_start
        
        # 2. ä» FAISS å‘é‡æ•°æ®åº“æ£€ç´¢
        # å€™é€‰æ–‡æ¡£ç­–ç•¥ï¼šå¦‚æœä½¿ç”¨é‡æ’åºï¼Œæ£€ç´¢ top_k Ã— 5 ä¸ªå€™é€‰æ–‡æ¡£
        candidate_multiplier = 5  # å€™é€‰æ–‡æ¡£å€æ•°ï¼ˆä¸ VLLM è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
        use_reranker = models.reranker_model is not None
        
        if use_reranker:
            # ä½¿ç”¨é‡æ’åºï¼šæ£€ç´¢æ›´å¤šå€™é€‰æ–‡æ¡£
            retrieval_k = min(top_k * candidate_multiplier, models.vector_db.indexes[city].ntotal)
            print(f"ğŸ” Retrieving {retrieval_k} candidates (top_k={top_k} Ã— multiplier={candidate_multiplier}) for reranking")
        else:
            # ä¸ä½¿ç”¨é‡æ’åºï¼šç›´æ¥æ£€ç´¢ top_k ä¸ª
            retrieval_k = top_k
            print(f"ğŸ” Retrieving {retrieval_k} candidates (no reranking)")
        
        retrieval_start = time.time()
        retrieved_docs = models.vector_db.search(query_embedding, city=city, top_k=retrieval_k)
        retrieval_time = time.time() - retrieval_start
        
        if not retrieved_docs:
            return {
                "answer": f"æœªæ‰¾åˆ°ä¸ã€Œ{query}ã€ç›¸å…³çš„å•†æˆ·ä¿¡æ¯",
                "sources": [],
                "metrics": {
                    "latency_ms": (time.time() - start_time) * 1000,
                    "embedding_time_ms": embedding_time * 1000,
                    "retrieval_time_ms": retrieval_time * 1000
                },
                "processing_time": time.time() - start_time
            }
        
        # 2.5. è½¬æ¢ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå°† L2 è·ç¦»è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦ï¼‰
        # å‚è€ƒ VLLM ç³»ç»Ÿçš„ç›¸ä¼¼åº¦è½¬æ¢ç­–ç•¥
        if retrieved_docs:
            max_distance = max(doc.get('vector_score', 0) for doc in retrieved_docs)
            for i, doc in enumerate(retrieved_docs):
                distance = doc.get('vector_score', 0)
                # å°† L2 è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
                similarity_score = max(0.0, (max_distance - distance) / max_distance) if max_distance > 0 else 0.0
                doc['distance'] = float(distance)  # ä¿ç•™åŸå§‹ L2 è·ç¦»
                doc['similarity'] = float(similarity_score)  # è½¬æ¢åçš„ç›¸ä¼¼åº¦ (0-1)
                doc['rank'] = i + 1  # åŸå§‹æ£€ç´¢æ’å
                doc['original_rank'] = i + 1  # ä¿å­˜åŸå§‹æ’å
        
        # 3. ä½¿ç”¨ Reranker æ¨¡å‹é‡æ’åº
        rerank_time = 0
        if use_reranker and len(retrieved_docs) > 1:
            try:
                rerank_start = time.time()
                
                # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹ï¼ˆä½¿ç”¨æ›´ä¸°å¯Œçš„æ–‡æ¡£è¡¨ç¤ºï¼‰
                pairs = []
                for doc in retrieved_docs:
                    # å‚è€ƒ VLLM ç³»ç»Ÿçš„æ–‡æ¡£æ ¼å¼åŒ–ç­–ç•¥ï¼šåŒ…å«å¤šä¸ªå…³é”®å­—æ®µ
                    doc_text = _format_document_for_rerank(doc)
                    pairs.append([query, doc_text])
                
                # ä½¿ç”¨ Reranker é‡æ–°æ‰“åˆ† (ä½¿ç”¨ batch_size=1 é¿å… padding é—®é¢˜)
                rerank_scores = models.reranker_model.predict(pairs, batch_size=1)
                
                # æ›´æ–°åˆ†æ•°
                for doc, score in zip(retrieved_docs, rerank_scores):
                    doc["rerank_score"] = float(score)
                
                # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
                retrieved_docs = sorted(retrieved_docs, key=lambda x: x.get("rerank_score", 0), reverse=True)
                
                # æ›´æ–°æœ€ç»ˆæ’å
                for i, doc in enumerate(retrieved_docs):
                    doc['final_rank'] = i + 1
                
                rerank_time = time.time() - rerank_start
                print(f"ğŸ”„ Reranked {len(retrieved_docs)} documents in {rerank_time:.2f}s")
                
            except Exception as e:
                print(f"âš ï¸ Reranking failed: {e}, using vector scores only")
                # é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’å
                for i, doc in enumerate(retrieved_docs):
                    doc['final_rank'] = doc.get('rank', i + 1)
        else:
            # ä¸ä½¿ç”¨é‡æ’åºï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ’å
            for i, doc in enumerate(retrieved_docs):
                doc['final_rank'] = doc.get('rank', i + 1)
        
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å­—æ®µ
        if retrieved_docs:
            print(f"ğŸ“‹ First document fields: {list(retrieved_docs[0].keys())}")
            print(f"ğŸ“‹ Merchant name: {retrieved_docs[0].get('name', 'NOT FOUND')}")
        
        # 4. ä½¿ç”¨ LLM ç²¾æ’ï¼ˆä» rerank çš„ç»“æœä¸­é€‰å‡º top_k ä¸ªï¼‰
        llm_ranking_time = 0
        if use_llm_ranking and models.llm_ranker and len(retrieved_docs) > top_k:
            try:
                llm_start = time.time()
                print(f"ğŸ¤– LLM ranking: selecting {top_k} from {len(retrieved_docs)} candidates")
                retrieved_docs = await models.llm_ranker.select_top_k_async(
                    query=query,
                    candidates=retrieved_docs,
                    top_k=top_k,
                    city=city
                )
                llm_ranking_time = time.time() - llm_start
                print(f"âœ… LLM ranking completed in {llm_ranking_time:.2f}s")
            except Exception as e:
                print(f"âš ï¸ LLM ranking failed: {e}, using reranked results")
                retrieved_docs = retrieved_docs[:top_k]
        else:
            # ä¸ä½¿ç”¨ LLM ç²¾æ’ï¼Œç›´æ¥å– top_k
            if not use_llm_ranking:
                print(f"ğŸ“‹ LLM ranking disabled by request")
            elif not models.llm_ranker:
                print(f"âš ï¸ LLM ranker not initialized")
            retrieved_docs = retrieved_docs[:top_k]
        
        # 5. ç”Ÿæˆç­”æ¡ˆæ‘˜è¦ï¼ˆcity å·²ç»æ˜¯ä¸­æ–‡ï¼‰
        answer = f"åœ¨{city}æ‰¾åˆ°ç›¸å…³å•†æˆ·ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹ {len(retrieved_docs)} å®¶ï¼š"
        
        # 6. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "retrieved_count": len(retrieved_docs),
            "returned_count": len(retrieved_docs),
            "city": city,
            "latency_ms": (time.time() - start_time) * 1000,
            "embedding_time_ms": embedding_time * 1000,
            "retrieval_time_ms": retrieval_time * 1000,
            "rerank_time_ms": rerank_time * 1000 if use_reranker else 0,
            "llm_ranking_time_ms": llm_ranking_time * 1000,
            "used_reranker": use_reranker,
            "used_llm_ranking": use_llm_ranking and llm_ranking_time > 0,
            "candidate_multiplier": candidate_multiplier if use_reranker else 1
        }
        
        # è°ƒè¯•ï¼šæ‰“å°è¿”å›çš„å•†åº—åç§°
        print(f"ğŸ“¦ Returning {len(retrieved_docs)} merchants:")
        for i, doc in enumerate(retrieved_docs[:3], 1):  # åªæ‰“å°å‰3ä¸ª
            score_info = f"rerank={doc.get('rerank_score', 0):.4f}" if use_reranker else f"similarity={doc.get('similarity', 0):.4f}"
            llm_rank = f", llm_rank={doc.get('llm_rank', '-')}" if doc.get('llm_selected') else ""
            print(f"   {i}. {doc.get('name', 'NO_NAME')} ({score_info}, rank: {doc.get('original_rank', '?')}â†’{doc.get('final_rank', '?')}{llm_rank})")
        
        return {
            "answer": answer,
            "sources": retrieved_docs,
            "metrics": metrics,
            "processing_time": time.time() - start_time
        }
        
    except Exception as e:
        print(f"âŒ RAG search error: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


def _format_document_for_rerank(doc_info: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºé‡æ’åºï¼ˆå¢å¼ºç‰ˆï¼šä½¿ç”¨æ¸…æ™°çš„ä¸­æ–‡æ ‡ç­¾ï¼‰
    
    å¢å¼ºç‰ˆï¼šåœ¨ VLLM è„šæœ¬åŸºç¡€ä¸Šï¼Œå¼ºåˆ¶åŒ…å«åœ°ç†ä½ç½®ä¿¡æ¯ï¼ˆcity, district, business_area, landmarkï¼‰
    æ„å»ºåŒ…å«å¤šä¸ªå…³é”®å­—æ®µçš„ä¸°å¯Œæ–‡æœ¬è¡¨ç¤ºï¼Œæé«˜é‡æ’åºå‡†ç¡®æ€§
    
    æ ¼å¼ç¤ºä¾‹ï¼š
        åº—åï¼šæ˜Ÿå·´å…‹å’–å•¡ - ç±»å‹ï¼šé¤é¥®/å’–å•¡å… - åœ°å€ï¼šåŒ—äº¬å¸‚æœé˜³åŒºå»ºå›½é—¨å¤–å¤§è¡—1å· - åŸå¸‚ï¼šåŒ—äº¬ - åŒºåŸŸï¼šæœé˜³åŒº - å•†åœˆï¼šå›½è´¸
    
    Args:
        doc_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–åçš„æ–‡æ¡£æ–‡æœ¬ï¼ˆå¸¦ä¸­æ–‡æ ‡ç­¾ï¼‰
    """
    parts = []
    
    # 1. åº—åï¼ˆå¿…å¡«ï¼‰
    if doc_info.get('name'):
        parts.append(f"åº—åï¼š{doc_info['name']}")
    
    # 2. ç±»å‹ï¼ˆç±»åˆ« + å­ç±»åˆ«ï¼‰
    category_parts = []
    if doc_info.get('category'):
        category_parts.append(doc_info['category'])
    if doc_info.get('subcategory'):
        category_parts.append(doc_info['subcategory'])
    
    if category_parts:
        parts.append(f"ç±»å‹ï¼š{'/'.join(category_parts)}")
    
    # 3. åœ°å€ï¼ˆå¿…å¡«ï¼‰
    if doc_info.get('address'):
        parts.append(f"åœ°å€ï¼š{doc_info['address']}")
    
    # 4. ğŸ”¥ åœ°ç†ä½ç½®ä¿¡æ¯ï¼ˆå¿…é¡»å‚ä¸é‡æ’ï¼‰
    if doc_info.get('city'):
        parts.append(f"åŸå¸‚ï¼š{doc_info['city']}")
    
    if doc_info.get('district'):
        parts.append(f"åŒºåŸŸï¼š{doc_info['district']}")
    
    if doc_info.get('business_area'):
        parts.append(f"å•†åœˆï¼š{doc_info['business_area']}")
    
    if doc_info.get('landmark'):
        parts.append(f"åœ°æ ‡ï¼š{doc_info['landmark']}")
    
    # ä½¿ç”¨ " - " è¿æ¥æ‰€æœ‰éƒ¨åˆ†
    return ' - '.join(parts)

def perform_web_search(query: str, top_k: int) -> Dict:
    """ä¼ ç»Ÿ Web æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Web æœç´¢é€»è¾‘
    # ä¾‹å¦‚ï¼šElasticSearch, BM25 ç­‰
    
    results = [
        {
            "merchant_name": f"å•†æˆ· {i+1}",
            "address": f"ä¸Šæµ·å¸‚æŸåŒºæŸè¡—é“{i+1}å·",
            "rating": 4.5 - i * 0.1,
            "price": f"äººå‡{100 + i*20}å…ƒ"
        }
        for i in range(top_k)
    ]
    
    return {
        "answer": f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ",
        "sources": results,
        "metrics": {"latency_ms": (time.time() - start_time) * 1000},
        "processing_time": time.time() - start_time
    }

def perform_agentic_search(query: str, model: str, max_iterations: int) -> Dict:
    """æ™ºèƒ½ä½“æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Agent é€»è¾‘
    # ä¾‹å¦‚ï¼šä½¿ç”¨ LangChain, AutoGPT ç­‰
    
    reasoning_steps = [
        "ğŸ¤” åˆ†ææŸ¥è¯¢æ„å›¾...",
        "ğŸ” ç¬¬1æ­¥ï¼šæœç´¢ç›¸å…³å•†æˆ·...",
        "ğŸ“Š ç¬¬2æ­¥ï¼šè¿‡æ»¤å’Œæ’åºç»“æœ...",
        "ğŸ’¡ ç¬¬3æ­¥ï¼šç”Ÿæˆæ¨è..."
    ]
    
    results = [
        {
            "merchant_name": "æ¨èå•†æˆ· 1",
            "address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
            "rating": 4.8,
            "reason": "é«˜è¯„åˆ†ä¸”ç¬¦åˆæ‚¨çš„éœ€æ±‚"
        }
    ]
    
    return {
        "answer": "åŸºäºå¤šæ­¥æ¨ç†ï¼Œä¸ºæ‚¨æ¨è...",
        "sources": results,
        "metrics": {
            "correctness": 0.85,
            "completeness": 0.90,
            "faithfulness": 0.88,
            "latency_ms": (time.time() - start_time) * 1000
        },
        "reasoning_steps": reasoning_steps,
        "processing_time": time.time() - start_time
    }

# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
def root():
    return {
        "service": "LocalSearchBench RAG API",
        "version": "1.0.0",
        "device": DEVICE,
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    cities_loaded = {}
    if models and models.vector_db:
        # CityVectorDB ä½¿ç”¨ city_to_en æ˜ å°„ï¼ˆä¸­æ–‡ -> è‹±æ–‡ï¼‰
        # indexes å’Œ metadata çš„ key æ˜¯ä¸­æ–‡åŸå¸‚å
        for city_cn, city_en in models.vector_db.city_to_en.items():
            if city_cn in models.vector_db.indexes:
                cities_loaded[city_en] = {
                "name": city_cn,
                    "vectors": models.vector_db.indexes[city_cn].ntotal,
                    "merchants": len(models.vector_db.metadata.get(city_cn, []))
        }
    
    return {
        "status": "healthy",
        "device": DEVICE,
        "gpu_available": torch.cuda.is_available() if 'torch' in globals() else False,
        "models_loaded": {
            "embedding": models.embedding_model is not None if models else False,
            "reranker": models.reranker_model is not None if models else False,
            "vector_db": models.vector_db is not None if models else False
        },
        "cities": cities_loaded,
        "total_cities": len(cities_loaded)
    }

@app.post("/api/rag/search", response_model=SearchResult)
async def rag_search(request: RAGSearchRequest):
    """RAG æœç´¢ç«¯ç‚¹ï¼ˆæ”¯æŒå¤šåŸå¸‚ï¼‰"""
    try:
        result = await perform_rag_search(
            query=request.query,
            city=request.city,
            top_k=request.top_k,
            retriever=request.retriever,
            reranker=request.reranker,
            use_llm_ranking=request.use_llm_ranking
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/web/search", response_model=SearchResult)
async def web_search(request: WebSearchRequest):
    """Web æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_web_search(
            query=request.query,
            top_k=request.top_k
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/agentic/search", response_model=SearchResult)
async def agentic_search(request: AgenticSearchRequest):
    """Agentic æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_agentic_search(
            query=request.query,
            model=request.model,
            max_iterations=request.max_iterations
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å’Œå‘é‡æ•°æ®åº“"""
    global models
    
    print("ğŸš€ Starting LocalSearchBench RAG Server...")
    print(f"ğŸ“ Device: {DEVICE}")
    
    # è·å–é…ç½®
    data_dir = getattr(app.state, 'data_dir', None)
    embedding_model_path = getattr(app.state, 'embedding_model_path', None)
    reranker_model_path = getattr(app.state, 'reranker_model_path', None)
    use_gpu = getattr(app.state, 'use_gpu', True)  # é»˜è®¤ä½¿ç”¨ GPU
    config_path = getattr(app.state, 'config_path', None)  # LLM é…ç½®æ–‡ä»¶è·¯å¾„
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŒ…æ‹¬å‘é‡æ•°æ®åº“å’Œ LLM ç²¾æ’å™¨ï¼‰
    models = RAGModels(data_dir=data_dir, use_gpu=use_gpu, config_path=config_path)
    
    # é¢„åŠ è½½æ¨¡å‹åˆ° GPU
    if DEVICE == "cuda":
        print("\nğŸ“¥ Pre-loading models to GPU...")
        
        # åŠ è½½ Embedding æ¨¡å‹
        if embedding_model_path:
            models.load_embedding_model(embedding_model_path)
        else:
            print("âš ï¸  No embedding model path specified, using default")
            models.load_embedding_model()
        
        # åŠ è½½ Reranker æ¨¡å‹
        if reranker_model_path:
            models.load_reranker_model(reranker_model_path)
        else:
            print("âš ï¸  No reranker model path specified, using default")
            models.load_reranker_model()
        
        print("âœ… Models loaded successfully")
    else:
        print("âš ï¸ Running in CPU mode")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
    if models.vector_db:
        print(f"\nâœ… Vector databases ready: {len(models.vector_db.indexes)} cities loaded")
    else:
        print("\nâš ï¸  No vector databases loaded. Please specify --data-dir")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ‘‹ Shutting down LocalSearchBench RAG Server...")
    # æ¸…ç† GPU æ˜¾å­˜
    if DEVICE == "cuda" and 'torch' in globals():
        torch.cuda.empty_cache()

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="LocalSearchBench RAG Server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--data-dir", type=str, default=None, help="Path to vector database directory (containing 1028 FAISS files)")
    parser.add_argument("--embedding-model", type=str, default=None, help="Path to Qwen3-Embedding-8B model")
    parser.add_argument("--reranker-model", type=str, default=None, help="Path to Qwen3-Reranker-8B model")
    parser.add_argument("--config", type=str, default=None, help="Path to config.yaml for LLM ranking")
    parser.add_argument("--use-gpu", action="store_true", default=True, help="Use GPU for FAISS vector search (default: True)")
    parser.add_argument("--no-gpu", action="store_true", help="Force CPU mode for FAISS vector search")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    args = parser.parse_args()
    
    # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    data_dir = args.data_dir or os.getenv("RAG_DATA_DIR")
    embedding_model_path = args.embedding_model or os.getenv("EMBEDDING_MODEL_PATH")
    reranker_model_path = args.reranker_model or os.getenv("RERANKER_MODEL_PATH")
    config_path = args.config or os.getenv("TUANSOU_CONFIG") or os.getenv("CONFIG_PATH")
    
    # GPU é…ç½®
    use_gpu = args.use_gpu and not args.no_gpu
    
    # å°†é…ç½®ä¿å­˜åˆ°å…¨å±€å˜é‡ä¾› startup_event ä½¿ç”¨
    app.state.data_dir = data_dir
    app.state.embedding_model_path = embedding_model_path
    app.state.reranker_model_path = reranker_model_path
    app.state.use_gpu = use_gpu
    app.state.config_path = config_path
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     LocalSearchBench RAG Server (Multi-City Support)      â•‘
â•‘     Device: {DEVICE:48s} â•‘
â•‘     Host: {args.host:50s} â•‘
â•‘     Port: {args.port:50d} â•‘
â•‘     Data Dir: {(data_dir or 'Not specified')[:45]:45s} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(
        app,  # ç›´æ¥ä¼ å…¥ app å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        host=args.host,
        port=args.port,
        reload=args.reload,
        workers=args.workers,
        log_level="info"
    )

if __name__ == "__main__":
    main()

