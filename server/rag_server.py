"""
RAG Server - éƒ¨ç½²åœ¨æœ‰ GPU çš„æœåŠ¡å™¨ä¸Š
æ”¯æŒ Web Searchã€RAG Search å’Œ Agentic Search

è¿è¡Œæ–¹å¼ï¼š
    python rag_server.py --port 8000 --host 0.0.0.0

ç¯å¢ƒå˜é‡é…ç½®ï¼š
    export OPENAI_API_KEY="your-key"
    export DASHSCOPE_API_KEY="your-key"  # å¦‚æœä½¿ç”¨ Qwen æ¨¡å‹
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
import argparse
import os
import time
from datetime import datetime

# åŸºç¡€ä¾èµ–
import json
import numpy as np

# å¦‚æœä½¿ç”¨ GPU åŠ è½½æ¨¡å‹
try:
    import torch
    from sentence_transformers import SentenceTransformer, CrossEncoder
    import faiss
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {DEVICE}")
    HAS_GPU = torch.cuda.is_available()
except ImportError as e:
    DEVICE = "cpu"
    HAS_GPU = False
    print(f"âš ï¸ PyTorch/FAISS not found: {e}, using CPU mode")

app = FastAPI(
    title="LocalSearchBench RAG API",
    description="RAG Search API with GPU support",
    version="1.0.0"
)

# é…ç½® CORS - å…è®¸ Gradio å®¢æˆ·ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================

class RAGSearchRequest(BaseModel):
    query: str
    city: str = "ä¸Šæµ·"  # æ”¯æŒçš„åŸå¸‚ï¼ˆä¸­æ–‡ï¼‰
    top_k: int = 10  # æœ€ç»ˆè¿”å›10ä¸ªç»“æœ
    retriever: str = "qwen3-embedding-8b"  # é»˜è®¤ä½¿ç”¨ Qwen3-Embedding-8B
    reranker: str = "qwen3-reranker-8b"    # é»˜è®¤ä½¿ç”¨ Qwen3-Reranker-8B

class WebSearchRequest(BaseModel):
    query: str
    top_k: int = 10

class AgenticSearchRequest(BaseModel):
    query: str
    model: str = "gpt-4.1"
    max_iterations: int = 5

class SearchResult(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, Any]  # æ”¹ä¸º Any ä»¥æ”¯æŒæ··åˆç±»å‹ï¼ˆfloatã€intã€strï¼‰
    reasoning_steps: Optional[List[str]] = None
    processing_time: float

# ==================== åŸå¸‚å‘é‡æ•°æ®åº“åŠ è½½å™¨ ====================

class CityVectorDB:
    """ç®¡ç†æ‰€æœ‰åŸå¸‚çš„FAISSå‘é‡æ•°æ®åº“ï¼ˆ1028ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, data_dir: str, use_gpu: bool = True):
        self.data_dir = data_dir
        self.use_gpu = use_gpu and torch.cuda.is_available()
        # åŸå¸‚æ˜ å°„ï¼šä¸­æ–‡ -> è‹±æ–‡ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        self.city_to_en = {
            "ä¸Šæµ·": "shanghai",
            "åŒ—äº¬": "beijing",
            "å¹¿å·": "guangzhou",
            "æ·±åœ³": "shenzhen",
            "æ­å·": "hangzhou",
            "è‹å·": "suzhou",
            "æˆéƒ½": "chengdu",
            "é‡åº†": "chongqing",
            "æ­¦æ±‰": "wuhan"
        }
        self.indexes = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.metadata = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.gpu_resources = None
        
        # åˆå§‹åŒ– GPU èµ„æº
        # æ³¨æ„ï¼šGPU å…¼å®¹æ€§æ£€æŸ¥åº”è¯¥åœ¨å¯åŠ¨è„šæœ¬ä¸­å®Œæˆï¼ˆstart_rag_server.shï¼‰
        # å› ä¸º FAISS çš„ C++ æ–­è¨€å¤±è´¥ä¼šå¯¼è‡´è¿›ç¨‹å´©æºƒï¼ŒPython æ— æ³•æ•è·
        if self.use_gpu:
            try:
                self.gpu_resources = faiss.StandardGpuResources()
                print(f"ğŸš€ GPU resources initialized for FAISS")
            except Exception as e:
                print(f"âš ï¸  Failed to initialize GPU resources: {e}")
                print(f"âš ï¸  Falling back to CPU mode")
                self.use_gpu = False
                self.gpu_resources = None
        
        self.load_all_cities()
    
    def load_all_cities(self):
        """åŠ è½½æ‰€æœ‰åŸå¸‚çš„å‘é‡æ•°æ®åº“"""
        device_info = "GPU" if self.use_gpu else "CPU"
        print(f"\nğŸ“¦ Loading vector databases from: {self.data_dir}")
        print(f"ğŸ’» Device: {device_info}")
        
        for city_cn, city_en in self.city_to_en.items():
            try:
                # åŠ è½½ 1028 ç‰ˆæœ¬çš„æ•°æ®ï¼ˆæ–‡ä»¶åä½¿ç”¨è‹±æ–‡ï¼‰
                index_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028.faiss")
                meta_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028_metadata.json")
                
                if not os.path.exists(index_path) or not os.path.exists(meta_path):
                    print(f"âš ï¸  {city_cn}: Files not found")
                    continue
                
                # åŠ è½½ FAISS ç´¢å¼• (å…ˆåŠ è½½åˆ°CPU)
                cpu_index = faiss.read_index(index_path)
                
                # å¦‚æœå¯ç”¨GPUï¼Œå°†ç´¢å¼•è½¬ç§»åˆ°GPU
                if self.use_gpu:
                    try:
                        # å°†CPUç´¢å¼•è½¬æ¢ä¸ºGPUç´¢å¼•ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                        self.indexes[city_cn] = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
                        device_tag = "ğŸš€ GPU"
                    except Exception as e:
                        print(f"âš ï¸  {city_cn}: GPU transfer failed ({e}), using CPU")
                        self.indexes[city_cn] = cpu_index
                        device_tag = "ğŸ’» CPU"
                else:
                    self.indexes[city_cn] = cpu_index
                    device_tag = "ğŸ’» CPU"
                
                # åŠ è½½å…ƒæ•°æ®ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                with open(meta_path, "r", encoding="utf-8") as f:
                    self.metadata[city_cn] = json.load(f)
                
                print(f"âœ… {city_cn}: {self.indexes[city_cn].ntotal} vectors, {len(self.metadata[city_cn])} merchants [{device_tag}]")
            except Exception as e:
                print(f"âŒ Failed to load {city_cn}: {e}")
        
        print(f"\nğŸ‰ Loaded {len(self.indexes)}/{len(self.city_to_en)} cities successfully on {device_info}!\n")
    
    def search(self, query_embedding: np.ndarray, city: str = "ä¸Šæµ·", top_k: int = 20):
        """åœ¨æŒ‡å®šåŸå¸‚çš„å‘é‡æ•°æ®åº“ä¸­æœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            city: åŸå¸‚åï¼ˆä¸­æ–‡ï¼‰ï¼Œå¦‚ "ä¸Šæµ·"ã€"åŒ—äº¬"
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        if city not in self.indexes:
            raise ValueError(f"City '{city}' not loaded. Available cities: {list(self.indexes.keys())}")
        
        # ä½¿ç”¨ FAISS è¿›è¡Œå‘é‡æ£€ç´¢
        query_vec = query_embedding.reshape(1, -1).astype('float32')
        distances, indices = self.indexes[city].search(query_vec, top_k)
        
        # è·å–å¯¹åº”çš„å…ƒæ•°æ®
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.metadata[city]):
                merchant = self.metadata[city][idx].copy()
                merchant["vector_score"] = float(dist)
                results.append(merchant)
        
        return results

# ==================== æ¨¡å‹åŠ è½½ï¼ˆGPUï¼‰====================

class RAGModels:
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹åˆ° GPU"""
    
    def __init__(self, data_dir: str = None, use_gpu: bool = True):
        self.embedding_model = None
        self.reranker_model = None
        self.llm = None
        self.vector_db = None
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒ GPU åŠ é€Ÿï¼‰
        if data_dir and os.path.exists(data_dir):
            try:
                self.vector_db = CityVectorDB(data_dir, use_gpu=use_gpu)
            except Exception as e:
                print(f"âš ï¸ Failed to load vector databases: {e}")
        
    def load_embedding_model(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """åŠ è½½ Embedding æ¨¡å‹åˆ° GPU"""
        if self.embedding_model is None:
            print(f"ğŸ“¥ Loading embedding model: {model_name}")
            # è¿™é‡Œä½¿ç”¨ sentence-transformers ä½œä¸ºç¤ºä¾‹
            # ä½ å¯ä»¥æ›¿æ¢ä¸º Qwen3-Embedding-8B æˆ–å…¶ä»–æ¨¡å‹
            try:
                self.embedding_model = SentenceTransformer(model_name, device=DEVICE)
                print(f"âœ… Embedding model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load embedding model: {e}")
                self.embedding_model = None
        return self.embedding_model
    
    def load_reranker_model(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """åŠ è½½ Reranker æ¨¡å‹åˆ° GPU"""
        if self.reranker_model is None:
            print(f"ğŸ“¥ Loading reranker model: {model_name}")
            try:
                from sentence_transformers import CrossEncoder
                self.reranker_model = CrossEncoder(model_name, device=DEVICE)
                print(f"âœ… Reranker model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load reranker model: {e}")
                self.reranker_model = None
        return self.reranker_model
    
    def encode_query(self, query: str):
        """ä½¿ç”¨ GPU è¿›è¡ŒæŸ¥è¯¢ç¼–ç """
        if self.embedding_model is None:
            self.load_embedding_model()
        
        if self.embedding_model:
            with torch.no_grad():
                embedding = self.embedding_model.encode(query, convert_to_tensor=True)
            return embedding.cpu().numpy()
        else:
            # Fallback: ä½¿ç”¨ç®€å•çš„æ–¹æ³•
            return None

# å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆç¨ååœ¨ startup æ—¶åˆå§‹åŒ–ï¼‰
models = None

# ==================== RAG å®ç° ====================

def perform_rag_search(query: str, city: str, top_k: int, retriever: str, reranker: str) -> Dict:
    """
    çœŸå®çš„ RAG æœç´¢å®ç°ï¼ˆä½¿ç”¨1028ç‰ˆæœ¬å‘é‡æ•°æ®åº“ï¼‰
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
    2. åœ¨æŒ‡å®šåŸå¸‚çš„ FAISS ç´¢å¼•ä¸­æ£€ç´¢
    3. ä½¿ç”¨ Reranker æ¨¡å‹é‡æ’åº
    4. è¿”å› top_k ç»“æœ
    """
    start_time = time.time()
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åŠ è½½
    if not models.vector_db:
        raise HTTPException(status_code=503, detail="Vector database not loaded. Please check server configuration.")
    
    if city not in models.vector_db.indexes:
        available_cities = list(models.vector_db.indexes.keys())
        raise HTTPException(
            status_code=400, 
            detail=f"City '{city}' not available. Available cities: {available_cities}"
        )
    
    try:
        # 1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
        query_embedding = models.encode_query(query)
        if query_embedding is None:
            raise HTTPException(status_code=503, detail="Embedding model not loaded")
        
        # 2. ä» FAISS å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆæ£€ç´¢20ä¸ªç”¨äºé‡æ’åºï¼‰
        retrieval_k = 50  # å›ºå®šæ£€ç´¢20ä¸ªå•†å®¶ç”¨äºé‡æ’åº
        retrieved_docs = models.vector_db.search(query_embedding, city=city, top_k=retrieval_k)
        
        if not retrieved_docs:
            return {
                "answer": f"æœªæ‰¾åˆ°ä¸ã€Œ{query}ã€ç›¸å…³çš„å•†æˆ·ä¿¡æ¯",
                "sources": [],
                "metrics": {"latency_ms": (time.time() - start_time) * 1000},
                "processing_time": time.time() - start_time
            }
        
        # 3. ä½¿ç”¨ Reranker æ¨¡å‹é‡æ’åº
        if models.reranker_model and len(retrieved_docs) > 1:
            try:
                # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
                pairs = []
                for doc in retrieved_docs:
                    # æ„å»ºæ›´ä¸°å¯Œçš„æ–‡æ¡£è¡¨ç¤º
                    doc_text = f"{doc.get('name', '')} {doc.get('description', '')} {doc.get('address', '')}"
                    pairs.append([query, doc_text])
                
                # ä½¿ç”¨ Reranker é‡æ–°æ‰“åˆ† (ä½¿ç”¨ batch_size=1 é¿å… padding é—®é¢˜)
                rerank_scores = models.reranker_model.predict(pairs, batch_size=1)
                
                # æ›´æ–°åˆ†æ•°å¹¶æ’åº
                for doc, score in zip(retrieved_docs, rerank_scores):
                    doc["rerank_score"] = float(score)
                
                retrieved_docs = sorted(retrieved_docs, key=lambda x: x.get("rerank_score", 0), reverse=True)
                print(f"ğŸ”„ Reranked {len(retrieved_docs)} documents")
                
            except Exception as e:
                print(f"âš ï¸ Reranking failed: {e}, using vector scores only")
        
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å­—æ®µ
        if retrieved_docs:
            print(f"ğŸ“‹ First document fields: {list(retrieved_docs[0].keys())}")
            print(f"ğŸ“‹ Merchant name: {retrieved_docs[0].get('name', 'NOT FOUND')}")
        
        # 4. ç”Ÿæˆç­”æ¡ˆæ‘˜è¦ï¼ˆcity å·²ç»æ˜¯ä¸­æ–‡ï¼‰
        answer = f"åœ¨{city}æ‰¾åˆ° {len(retrieved_docs)} å®¶ç›¸å…³å•†æˆ·ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹ {min(top_k, len(retrieved_docs))} å®¶ï¼š"
        
        # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "retrieved_count": len(retrieved_docs),
            "returned_count": min(top_k, len(retrieved_docs)),
            "city": city,
            "latency_ms": (time.time() - start_time) * 1000
        }
        
        # è°ƒè¯•ï¼šæ‰“å°è¿”å›çš„å•†åº—åç§°
        top_merchants = retrieved_docs[:top_k]
        print(f"ğŸ“¦ Returning top {len(top_merchants)} merchants:")
        for i, doc in enumerate(top_merchants[:3], 1):  # åªæ‰“å°å‰3ä¸ª
            print(f"   {i}. {doc.get('name', 'NO_NAME')} (score: {doc.get('rerank_score', doc.get('vector_score', 0)):.4f})")
        
        return {
            "answer": answer,
            "sources": retrieved_docs[:top_k],
            "metrics": metrics,
            "processing_time": time.time() - start_time
        }
        
    except Exception as e:
        print(f"âŒ RAG search error: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")

def perform_web_search(query: str, top_k: int) -> Dict:
    """ä¼ ç»Ÿ Web æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Web æœç´¢é€»è¾‘
    # ä¾‹å¦‚ï¼šElasticSearch, BM25 ç­‰
    
    results = [
        {
            "merchant_name": f"å•†æˆ· {i+1}",
            "address": f"ä¸Šæµ·å¸‚æŸåŒºæŸè¡—é“{i+1}å·",
            "rating": 4.5 - i * 0.1,
            "price": f"äººå‡{100 + i*20}å…ƒ"
        }
        for i in range(top_k)
    ]
    
    return {
        "answer": f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ",
        "sources": results,
        "metrics": {"latency_ms": (time.time() - start_time) * 1000},
        "processing_time": time.time() - start_time
    }

def perform_agentic_search(query: str, model: str, max_iterations: int) -> Dict:
    """æ™ºèƒ½ä½“æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Agent é€»è¾‘
    # ä¾‹å¦‚ï¼šä½¿ç”¨ LangChain, AutoGPT ç­‰
    
    reasoning_steps = [
        "ğŸ¤” åˆ†ææŸ¥è¯¢æ„å›¾...",
        "ğŸ” ç¬¬1æ­¥ï¼šæœç´¢ç›¸å…³å•†æˆ·...",
        "ğŸ“Š ç¬¬2æ­¥ï¼šè¿‡æ»¤å’Œæ’åºç»“æœ...",
        "ğŸ’¡ ç¬¬3æ­¥ï¼šç”Ÿæˆæ¨è..."
    ]
    
    results = [
        {
            "merchant_name": "æ¨èå•†æˆ· 1",
            "address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
            "rating": 4.8,
            "reason": "é«˜è¯„åˆ†ä¸”ç¬¦åˆæ‚¨çš„éœ€æ±‚"
        }
    ]
    
    return {
        "answer": "åŸºäºå¤šæ­¥æ¨ç†ï¼Œä¸ºæ‚¨æ¨è...",
        "sources": results,
        "metrics": {
            "correctness": 0.85,
            "completeness": 0.90,
            "faithfulness": 0.88,
            "latency_ms": (time.time() - start_time) * 1000
        },
        "reasoning_steps": reasoning_steps,
        "processing_time": time.time() - start_time
    }

# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
def root():
    return {
        "service": "LocalSearchBench RAG API",
        "version": "1.0.0",
        "device": DEVICE,
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    cities_loaded = {}
    if models.vector_db:
        cities_loaded = {
            city_en: {
                "name": city_cn,
                "vectors": models.vector_db.indexes[city_en].ntotal if city_en in models.vector_db.indexes else 0,
                "merchants": len(models.vector_db.metadata.get(city_en, []))
            }
            for city_en, city_cn in models.vector_db.cities.items()
            if city_en in models.vector_db.indexes
        }
    
    return {
        "status": "healthy",
        "device": DEVICE,
        "gpu_available": torch.cuda.is_available() if 'torch' in globals() else False,
        "models_loaded": {
            "embedding": models.embedding_model is not None,
            "reranker": models.reranker_model is not None,
            "vector_db": models.vector_db is not None
        },
        "cities": cities_loaded,
        "total_cities": len(cities_loaded)
    }

@app.post("/api/rag/search", response_model=SearchResult)
async def rag_search(request: RAGSearchRequest):
    """RAG æœç´¢ç«¯ç‚¹ï¼ˆæ”¯æŒå¤šåŸå¸‚ï¼‰"""
    try:
        result = perform_rag_search(
            query=request.query,
            city=request.city,
            top_k=request.top_k,
            retriever=request.retriever,
            reranker=request.reranker
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/web/search", response_model=SearchResult)
async def web_search(request: WebSearchRequest):
    """Web æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_web_search(
            query=request.query,
            top_k=request.top_k
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/agentic/search", response_model=SearchResult)
async def agentic_search(request: AgenticSearchRequest):
    """Agentic æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_agentic_search(
            query=request.query,
            model=request.model,
            max_iterations=request.max_iterations
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å’Œå‘é‡æ•°æ®åº“"""
    global models
    
    print("ğŸš€ Starting LocalSearchBench RAG Server...")
    print(f"ğŸ“ Device: {DEVICE}")
    
    # è·å–é…ç½®
    data_dir = getattr(app.state, 'data_dir', None)
    embedding_model_path = getattr(app.state, 'embedding_model_path', None)
    reranker_model_path = getattr(app.state, 'reranker_model_path', None)
    use_gpu = getattr(app.state, 'use_gpu', True)  # é»˜è®¤ä½¿ç”¨ GPU
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŒ…æ‹¬å‘é‡æ•°æ®åº“ï¼Œä¼šæ ¹æ® use_gpu å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨ GPUï¼‰
    models = RAGModels(data_dir=data_dir, use_gpu=use_gpu)
    
    # é¢„åŠ è½½æ¨¡å‹åˆ° GPU
    if DEVICE == "cuda":
        print("\nğŸ“¥ Pre-loading models to GPU...")
        
        # åŠ è½½ Embedding æ¨¡å‹
        if embedding_model_path:
            models.load_embedding_model(embedding_model_path)
        else:
            print("âš ï¸  No embedding model path specified, using default")
            models.load_embedding_model()
        
        # åŠ è½½ Reranker æ¨¡å‹
        if reranker_model_path:
            models.load_reranker_model(reranker_model_path)
        else:
            print("âš ï¸  No reranker model path specified, using default")
            models.load_reranker_model()
        
        print("âœ… Models loaded successfully")
    else:
        print("âš ï¸ Running in CPU mode")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
    if models.vector_db:
        print(f"\nâœ… Vector databases ready: {len(models.vector_db.indexes)} cities loaded")
    else:
        print("\nâš ï¸  No vector databases loaded. Please specify --data-dir")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ‘‹ Shutting down LocalSearchBench RAG Server...")
    # æ¸…ç† GPU æ˜¾å­˜
    if DEVICE == "cuda" and 'torch' in globals():
        torch.cuda.empty_cache()

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="LocalSearchBench RAG Server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--data-dir", type=str, default=None, help="Path to vector database directory (containing 1028 FAISS files)")
    parser.add_argument("--embedding-model", type=str, default=None, help="Path to Qwen3-Embedding-8B model")
    parser.add_argument("--reranker-model", type=str, default=None, help="Path to Qwen3-Reranker-8B model")
    parser.add_argument("--use-gpu", action="store_true", default=True, help="Use GPU for FAISS vector search (default: True)")
    parser.add_argument("--no-gpu", action="store_true", help="Force CPU mode for FAISS vector search")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    args = parser.parse_args()
    
    # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    data_dir = args.data_dir or os.getenv("RAG_DATA_DIR")
    embedding_model_path = args.embedding_model or os.getenv("EMBEDDING_MODEL_PATH")
    reranker_model_path = args.reranker_model or os.getenv("RERANKER_MODEL_PATH")
    
    # GPU é…ç½®
    use_gpu = args.use_gpu and not args.no_gpu
    
    # å°†é…ç½®ä¿å­˜åˆ°å…¨å±€å˜é‡ä¾› startup_event ä½¿ç”¨
    app.state.data_dir = data_dir
    app.state.embedding_model_path = embedding_model_path
    app.state.reranker_model_path = reranker_model_path
    app.state.use_gpu = use_gpu
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     LocalSearchBench RAG Server (Multi-City Support)      â•‘
â•‘     Device: {DEVICE:48s} â•‘
â•‘     Host: {args.host:50s} â•‘
â•‘     Port: {args.port:50d} â•‘
â•‘     Data Dir: {(data_dir or 'Not specified')[:45]:45s} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(
        app,  # ç›´æ¥ä¼ å…¥ app å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        host=args.host,
        port=args.port,
        reload=args.reload,
        workers=args.workers,
        log_level="info"
    )

if __name__ == "__main__":
    main()

