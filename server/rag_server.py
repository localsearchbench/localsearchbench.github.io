"""
RAG Server - éƒ¨ç½²åœ¨æœ‰ GPU çš„æœåŠ¡å™¨ä¸Š
æ”¯æŒ Web Searchã€RAG Search å’Œ Agentic Search

è¿è¡Œæ–¹å¼ï¼š
    python rag_server.py --port 8000 --host 0.0.0.0

ç¯å¢ƒå˜é‡é…ç½®ï¼š
    export OPENAI_API_KEY="your-key"
    export DASHSCOPE_API_KEY="your-key"  # å¦‚æœä½¿ç”¨ Qwen æ¨¡å‹

æ£€ç´¢ä¸é‡æ’ç­–ç•¥ï¼š
    æœ¬æœåŠ¡å™¨ä¸ interactive_merchant_search_vllm.py ä¿æŒé«˜åº¦ä¸€è‡´ï¼š
    - å€™é€‰æ–‡æ¡£å€æ•°ï¼šcandidate_multiplier = 5
    - ç›¸ä¼¼åº¦è®¡ç®—ï¼š(max_distance - distance) / max_distance
    - é‡æ’åºæ–‡æœ¬æ ¼å¼ï¼šname - category/subcategory - address + åœ°ç†ä½ç½®ï¼ˆå¿…é¡»ï¼‰+ å¤šä¸ªå¯é€‰å­—æ®µ
    - åœ°ç†ä½ç½®å­—æ®µï¼ˆå¿…é¡»å‚ä¸é‡æ’ï¼‰ï¼šcity, district, business_area, landmark
    - subcategory å­—æ®µï¼šå¦‚æœå­˜åœ¨ï¼Œä¼šæ‹¼æ¥åˆ° category åé¢ï¼ˆæ ¼å¼ï¼šcategory/subcategoryï¼‰
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional, Any
import uvicorn
import argparse
import os
import time
from datetime import datetime

# åŸºç¡€ä¾èµ–
import json
import numpy as np

# å¦‚æœä½¿ç”¨ GPU åŠ è½½æ¨¡å‹
try:
    import torch
    from sentence_transformers import SentenceTransformer, CrossEncoder
    import faiss
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ Using device: {DEVICE}")
    HAS_GPU = torch.cuda.is_available()
except ImportError as e:
    DEVICE = "cpu"
    HAS_GPU = False
    print(f"âš ï¸ PyTorch/FAISS not found: {e}, using CPU mode")

app = FastAPI(
    title="LocalSearchBench RAG API",
    description="RAG Search API with GPU support",
    version="1.0.0"
)

# é…ç½® CORS - å…è®¸ Gradio å®¢æˆ·ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒå»ºè®®é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== æ•°æ®æ¨¡å‹ ====================

class RAGSearchRequest(BaseModel):
    query: str
    city: str = "ä¸Šæµ·"  # æ”¯æŒçš„åŸå¸‚ï¼ˆä¸­æ–‡ï¼‰
    top_k: int = 10  # æœ€ç»ˆè¿”å›10ä¸ªç»“æœ
    retriever: str = "qwen3-embedding-8b"  # é»˜è®¤ä½¿ç”¨ Qwen3-Embedding-8B
    reranker: str = "qwen3-reranker-8b"    # é»˜è®¤ä½¿ç”¨ Qwen3-Reranker-8B

class WebSearchRequest(BaseModel):
    query: str
    top_k: int = 10

class AgenticSearchRequest(BaseModel):
    query: str
    model: str = "gpt-4.1"
    max_iterations: int = 5

class SearchResult(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    metrics: Dict[str, Any]  # æ”¹ä¸º Any ä»¥æ”¯æŒæ··åˆç±»å‹ï¼ˆfloatã€intã€strï¼‰
    reasoning_steps: Optional[List[str]] = None
    processing_time: float

# ==================== åŸå¸‚å‘é‡æ•°æ®åº“åŠ è½½å™¨ ====================

class CityVectorDB:
    """ç®¡ç†æ‰€æœ‰åŸå¸‚çš„FAISSå‘é‡æ•°æ®åº“ï¼ˆ1028ç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, data_dir: str, use_gpu: bool = True):
        self.data_dir = data_dir
        self.use_gpu = use_gpu and torch.cuda.is_available()
        # åŸå¸‚æ˜ å°„ï¼šä¸­æ–‡ -> è‹±æ–‡ï¼ˆç”¨äºæ–‡ä»¶åï¼‰
        self.city_to_en = {
            "ä¸Šæµ·": "shanghai",
            "åŒ—äº¬": "beijing",
            "å¹¿å·": "guangzhou",
            "æ·±åœ³": "shenzhen",
            "æ­å·": "hangzhou",
            "è‹å·": "suzhou",
            "æˆéƒ½": "chengdu",
            "é‡åº†": "chongqing",
            "æ­¦æ±‰": "wuhan"
        }
        self.indexes = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.metadata = {}  # key ä¸ºä¸­æ–‡åŸå¸‚å
        self.gpu_resources = None
        
        # åˆå§‹åŒ– GPU èµ„æº
        # æ³¨æ„ï¼šGPU å…¼å®¹æ€§æ£€æŸ¥åº”è¯¥åœ¨å¯åŠ¨è„šæœ¬ä¸­å®Œæˆï¼ˆstart_rag_server.shï¼‰
        # å› ä¸º FAISS çš„ C++ æ–­è¨€å¤±è´¥ä¼šå¯¼è‡´è¿›ç¨‹å´©æºƒï¼ŒPython æ— æ³•æ•è·
        if self.use_gpu:
            try:
                self.gpu_resources = faiss.StandardGpuResources()
                print(f"ğŸš€ GPU resources initialized for FAISS")
            except Exception as e:
                print(f"âš ï¸  Failed to initialize GPU resources: {e}")
                print(f"âš ï¸  Falling back to CPU mode")
                self.use_gpu = False
                self.gpu_resources = None
        
        self.load_all_cities()
    
    def load_all_cities(self):
        """åŠ è½½æ‰€æœ‰åŸå¸‚çš„å‘é‡æ•°æ®åº“"""
        device_info = "GPU" if self.use_gpu else "CPU"
        print(f"\nğŸ“¦ Loading vector databases from: {self.data_dir}")
        print(f"ğŸ’» Device: {device_info}")
        
        for city_cn, city_en in self.city_to_en.items():
            try:
                # åŠ è½½ 1028 ç‰ˆæœ¬çš„æ•°æ®ï¼ˆæ–‡ä»¶åä½¿ç”¨è‹±æ–‡ï¼‰
                index_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028.faiss")
                meta_path = os.path.join(self.data_dir, f"faiss_merchant_index_vllm_{city_en}_1028_metadata.json")
                
                if not os.path.exists(index_path) or not os.path.exists(meta_path):
                    print(f"âš ï¸  {city_cn}: Files not found")
                    continue
                
                # åŠ è½½ FAISS ç´¢å¼• (å…ˆåŠ è½½åˆ°CPU)
                cpu_index = faiss.read_index(index_path)
                
                # å¦‚æœå¯ç”¨GPUï¼Œå°†ç´¢å¼•è½¬ç§»åˆ°GPU
                if self.use_gpu:
                    try:
                        # å°†CPUç´¢å¼•è½¬æ¢ä¸ºGPUç´¢å¼•ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                        self.indexes[city_cn] = faiss.index_cpu_to_gpu(self.gpu_resources, 0, cpu_index)
                        device_tag = "ğŸš€ GPU"
                    except Exception as e:
                        print(f"âš ï¸  {city_cn}: GPU transfer failed ({e}), using CPU")
                        self.indexes[city_cn] = cpu_index
                        device_tag = "ğŸ’» CPU"
                else:
                    self.indexes[city_cn] = cpu_index
                    device_tag = "ğŸ’» CPU"
                
                # åŠ è½½å…ƒæ•°æ®ï¼ˆä½¿ç”¨ä¸­æ–‡ä½œä¸º keyï¼‰
                with open(meta_path, "r", encoding="utf-8") as f:
                    self.metadata[city_cn] = json.load(f)
                
                print(f"âœ… {city_cn}: {self.indexes[city_cn].ntotal} vectors, {len(self.metadata[city_cn])} merchants [{device_tag}]")
            except Exception as e:
                print(f"âŒ Failed to load {city_cn}: {e}")
        
        print(f"\nğŸ‰ Loaded {len(self.indexes)}/{len(self.city_to_en)} cities successfully on {device_info}!\n")
    
    def search(self, query_embedding: np.ndarray, city: str = "ä¸Šæµ·", top_k: int = 20):
        """åœ¨æŒ‡å®šåŸå¸‚çš„å‘é‡æ•°æ®åº“ä¸­æœç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            city: åŸå¸‚åï¼ˆä¸­æ–‡ï¼‰ï¼Œå¦‚ "ä¸Šæµ·"ã€"åŒ—äº¬"
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        if city not in self.indexes:
            raise ValueError(f"City '{city}' not loaded. Available cities: {list(self.indexes.keys())}")
        
        # ä½¿ç”¨ FAISS è¿›è¡Œå‘é‡æ£€ç´¢
        query_vec = query_embedding.reshape(1, -1).astype('float32')
        distances, indices = self.indexes[city].search(query_vec, top_k)
        
        # è·å–å¯¹åº”çš„å…ƒæ•°æ®
        results = []
        for idx, dist in zip(indices[0], distances[0]):
            if idx < len(self.metadata[city]):
                merchant = self.metadata[city][idx].copy()
                merchant["vector_score"] = float(dist)
                results.append(merchant)
        
        return results

# ==================== æ¨¡å‹åŠ è½½ï¼ˆGPUï¼‰====================

class RAGModels:
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹åˆ° GPU"""
    
    def __init__(self, data_dir: str = None, use_gpu: bool = True):
        self.embedding_model = None
        self.reranker_model = None
        self.llm = None
        self.vector_db = None
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒ GPU åŠ é€Ÿï¼‰
        if data_dir and os.path.exists(data_dir):
            try:
                self.vector_db = CityVectorDB(data_dir, use_gpu=use_gpu)
            except Exception as e:
                print(f"âš ï¸ Failed to load vector databases: {e}")
        
    def load_embedding_model(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        """åŠ è½½ Embedding æ¨¡å‹åˆ° GPU"""
        if self.embedding_model is None:
            print(f"ğŸ“¥ Loading embedding model: {model_name}")
            # è¿™é‡Œä½¿ç”¨ sentence-transformers ä½œä¸ºç¤ºä¾‹
            # ä½ å¯ä»¥æ›¿æ¢ä¸º Qwen3-Embedding-8B æˆ–å…¶ä»–æ¨¡å‹
            try:
                self.embedding_model = SentenceTransformer(model_name, device=DEVICE)
                print(f"âœ… Embedding model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load embedding model: {e}")
                self.embedding_model = None
        return self.embedding_model
    
    def load_reranker_model(self, model_name: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"):
        """åŠ è½½ Reranker æ¨¡å‹åˆ° GPU"""
        if self.reranker_model is None:
            print(f"ğŸ“¥ Loading reranker model: {model_name}")
            try:
                from sentence_transformers import CrossEncoder
                self.reranker_model = CrossEncoder(model_name, device=DEVICE)
                print(f"âœ… Reranker model loaded on {DEVICE}")
            except Exception as e:
                print(f"âŒ Failed to load reranker model: {e}")
                self.reranker_model = None
        return self.reranker_model
    
    def encode_query(self, query: str):
        """ä½¿ç”¨ GPU è¿›è¡ŒæŸ¥è¯¢ç¼–ç """
        if self.embedding_model is None:
            self.load_embedding_model()
        
        if self.embedding_model:
            with torch.no_grad():
                embedding = self.embedding_model.encode(query, convert_to_tensor=True)
            return embedding.cpu().numpy()
        else:
            # Fallback: ä½¿ç”¨ç®€å•çš„æ–¹æ³•
            return None

# å…¨å±€æ¨¡å‹å®ä¾‹ï¼ˆç¨ååœ¨ startup æ—¶åˆå§‹åŒ–ï¼‰
models = None

# ==================== RAG å®ç° ====================

def perform_rag_search(query: str, city: str, top_k: int, retriever: str, reranker: str) -> Dict:
    """
    çœŸå®çš„ RAG æœç´¢å®ç°ï¼ˆä½¿ç”¨1028ç‰ˆæœ¬å‘é‡æ•°æ®åº“ï¼‰
    
    æµç¨‹ï¼š
    1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
    2. åœ¨æŒ‡å®šåŸå¸‚çš„ FAISS ç´¢å¼•ä¸­æ£€ç´¢ï¼ˆå€™é€‰æ–‡æ¡£æ•°é‡ = top_k Ã— candidate_multiplierï¼‰
    3. ä½¿ç”¨ Reranker æ¨¡å‹é‡æ’åº
    4. è¿”å› top_k ç»“æœ
    
    å‚è€ƒç­–ç•¥ï¼ˆä¸ interactive_merchant_search_vllm.py ä¿æŒä¸€è‡´ï¼‰ï¼š
    - å€™é€‰æ–‡æ¡£å€æ•°ï¼š5å€ï¼ˆå³æ£€ç´¢ top_k Ã— 5 ä¸ªå€™é€‰æ–‡æ¡£ï¼‰
    - ç›¸ä¼¼åº¦è½¬æ¢ï¼šå°† L2 è·ç¦»è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦åˆ†æ•°
    - é‡æ’åºæ–‡æœ¬ï¼šæ„å»ºåŒ…å«åœ°ç†ä½ç½®ï¼ˆcity/district/business_area/landmarkï¼‰+ å¤šä¸ªå…³é”®å­—æ®µçš„ä¸°å¯Œæ–‡æœ¬è¡¨ç¤º
    - ä¿ç•™æ’åä¿¡æ¯ï¼šè®°å½•åŸå§‹æ’åã€é‡æ’åºåˆ†æ•°å’Œæœ€ç»ˆæ’å
    """
    start_time = time.time()
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åŠ è½½
    if not models.vector_db:
        raise HTTPException(status_code=503, detail="Vector database not loaded. Please check server configuration.")
    
    if city not in models.vector_db.indexes:
        available_cities = list(models.vector_db.indexes.keys())
        raise HTTPException(
            status_code=400, 
            detail=f"City '{city}' not available. Available cities: {available_cities}"
        )
    
    try:
        # 1. ä½¿ç”¨ Embedding æ¨¡å‹ç¼–ç æŸ¥è¯¢
        embedding_start = time.time()
        query_embedding = models.encode_query(query)
        if query_embedding is None:
            raise HTTPException(status_code=503, detail="Embedding model not loaded")
        embedding_time = time.time() - embedding_start
        
        # 2. ä» FAISS å‘é‡æ•°æ®åº“æ£€ç´¢
        # å€™é€‰æ–‡æ¡£ç­–ç•¥ï¼šå¦‚æœä½¿ç”¨é‡æ’åºï¼Œæ£€ç´¢ top_k Ã— 5 ä¸ªå€™é€‰æ–‡æ¡£
        candidate_multiplier = 5  # å€™é€‰æ–‡æ¡£å€æ•°ï¼ˆä¸ VLLM è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
        use_reranker = models.reranker_model is not None
        
        if use_reranker:
            # ä½¿ç”¨é‡æ’åºï¼šæ£€ç´¢æ›´å¤šå€™é€‰æ–‡æ¡£
            retrieval_k = min(top_k * candidate_multiplier, models.vector_db.indexes[city].ntotal)
            print(f"ğŸ” Retrieving {retrieval_k} candidates (top_k={top_k} Ã— multiplier={candidate_multiplier}) for reranking")
        else:
            # ä¸ä½¿ç”¨é‡æ’åºï¼šç›´æ¥æ£€ç´¢ top_k ä¸ª
            retrieval_k = top_k
            print(f"ğŸ” Retrieving {retrieval_k} candidates (no reranking)")
        
        retrieval_start = time.time()
        retrieved_docs = models.vector_db.search(query_embedding, city=city, top_k=retrieval_k)
        retrieval_time = time.time() - retrieval_start
        
        if not retrieved_docs:
            return {
                "answer": f"æœªæ‰¾åˆ°ä¸ã€Œ{query}ã€ç›¸å…³çš„å•†æˆ·ä¿¡æ¯",
                "sources": [],
                "metrics": {
                    "latency_ms": (time.time() - start_time) * 1000,
                    "embedding_time_ms": embedding_time * 1000,
                    "retrieval_time_ms": retrieval_time * 1000
                },
                "processing_time": time.time() - start_time
            }
        
        # 2.5. è½¬æ¢ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå°† L2 è·ç¦»è½¬æ¢ä¸º 0-1 èŒƒå›´çš„ç›¸ä¼¼åº¦ï¼‰
        # å‚è€ƒ VLLM ç³»ç»Ÿçš„ç›¸ä¼¼åº¦è½¬æ¢ç­–ç•¥
        if retrieved_docs:
            max_distance = max(doc.get('vector_score', 0) for doc in retrieved_docs)
            for i, doc in enumerate(retrieved_docs):
                distance = doc.get('vector_score', 0)
                # å°† L2 è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
                similarity_score = max(0.0, (max_distance - distance) / max_distance) if max_distance > 0 else 0.0
                doc['distance'] = float(distance)  # ä¿ç•™åŸå§‹ L2 è·ç¦»
                doc['similarity'] = float(similarity_score)  # è½¬æ¢åçš„ç›¸ä¼¼åº¦ (0-1)
                doc['rank'] = i + 1  # åŸå§‹æ£€ç´¢æ’å
                doc['original_rank'] = i + 1  # ä¿å­˜åŸå§‹æ’å
        
        # 3. ğŸ”¥ ä¸¤é˜¶æ®µé‡æ’åºç­–ç•¥ï¼šå…ˆåœ°ç†è¿‡æ»¤ï¼Œå†ç±»å‹åŒ¹é…
        rerank_time = 0
        if use_reranker and len(retrieved_docs) > 1:
            try:
                rerank_start = time.time()
                
                # ğŸ”¥ é˜¶æ®µ1ï¼šæå–æŸ¥è¯¢ä¸­çš„åœ°ç†ä½ç½®å…³é”®è¯
                location_keywords = _extract_location_from_query(query)
                print(f"ğŸ—ºï¸  Extracted location keywords: {location_keywords}")
                
                # ğŸ”¥ é˜¶æ®µ2ï¼šè®¡ç®—åœ°ç†ç›¸å…³æ€§åˆ†æ•°
                for doc in retrieved_docs:
                    location_score = _calculate_location_relevance(doc, location_keywords)
                    doc['location_score'] = location_score
                
                # ğŸ”¥ é˜¶æ®µ3ï¼šå¦‚æœæœ‰åœ°ç†å…³é”®è¯ï¼Œå…ˆæŒ‰åœ°ç†ç›¸å…³æ€§è¿‡æ»¤
                if location_keywords:
                    # ç»Ÿè®¡åœ°ç†åŒ¹é…æƒ…å†µ
                    matched_docs = [doc for doc in retrieved_docs if doc.get('location_score', 0) > 0]
                    unmatched_docs = [doc for doc in retrieved_docs if doc.get('location_score', 0) == 0]
                    
                    print(f"ğŸ“ Location filtering: {len(matched_docs)} matched, {len(unmatched_docs)} unmatched")
                    
                    # å¦‚æœæœ‰åŒ¹é…åœ°ç†ä½ç½®çš„æ–‡æ¡£ï¼Œä¼˜å…ˆä½¿ç”¨å®ƒä»¬
                    if matched_docs:
                        # å¯¹åŒ¹é…åœ°ç†ä½ç½®çš„æ–‡æ¡£è¿›è¡Œé‡æ’åº
                        pairs = []
                        for doc in matched_docs:
                            doc_text = _format_document_for_rerank(doc)
                            pairs.append([query, doc_text])
                        
                        # ä½¿ç”¨ Reranker é‡æ–°æ‰“åˆ†
                        rerank_scores = models.reranker_model.predict(pairs, batch_size=1)
                        
                        # æ›´æ–°åˆ†æ•°ï¼ˆåœ°ç†åˆ†æ•° Ã— 0.3 + rerankåˆ†æ•° Ã— 0.7ï¼‰
                        for doc, score in zip(matched_docs, rerank_scores):
                            doc["rerank_score"] = float(score)
                            # ğŸ”¥ ç»¼åˆåˆ†æ•°ï¼šåœ°ç†ä½ç½®æƒé‡30%ï¼Œè¯­ä¹‰ç›¸å…³æ€§æƒé‡70%
                            doc["final_score"] = doc['location_score'] * 0.3 + float(score) * 0.7
                        
                        # å¯¹æœªåŒ¹é…åœ°ç†ä½ç½®çš„æ–‡æ¡£ä¹Ÿæ‰“åˆ†ï¼ˆä½†åˆ†æ•°é™ä½ï¼‰
                        if unmatched_docs:
                            pairs_unmatched = []
                            for doc in unmatched_docs:
                                doc_text = _format_document_for_rerank(doc)
                                pairs_unmatched.append([query, doc_text])
                            
                            rerank_scores_unmatched = models.reranker_model.predict(pairs_unmatched, batch_size=1)
                            for doc, score in zip(unmatched_docs, rerank_scores_unmatched):
                                doc["rerank_score"] = float(score)
                                # ğŸ”¥ æœªåŒ¹é…åœ°ç†ä½ç½®çš„æ–‡æ¡£åˆ†æ•°é™ä½ï¼ˆÃ— 0.5ï¼‰
                                doc["final_score"] = float(score) * 0.5
                        
                        # åˆå¹¶å¹¶æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                        retrieved_docs = matched_docs + unmatched_docs
                        retrieved_docs = sorted(retrieved_docs, key=lambda x: x.get("final_score", 0), reverse=True)
                    else:
                        # å¦‚æœæ²¡æœ‰åŒ¹é…åœ°ç†ä½ç½®çš„æ–‡æ¡£ï¼Œä½¿ç”¨åŸå§‹é‡æ’åºé€»è¾‘
                        print("âš ï¸  No location-matched documents, using standard reranking")
                        pairs = []
                        for doc in retrieved_docs:
                            doc_text = _format_document_for_rerank(doc)
                            pairs.append([query, doc_text])
                        
                        rerank_scores = models.reranker_model.predict(pairs, batch_size=1)
                        for doc, score in zip(retrieved_docs, rerank_scores):
                            doc["rerank_score"] = float(score)
                            doc["final_score"] = float(score)
                        
                        retrieved_docs = sorted(retrieved_docs, key=lambda x: x.get("final_score", 0), reverse=True)
                else:
                    # æ²¡æœ‰åœ°ç†å…³é”®è¯ï¼Œä½¿ç”¨æ ‡å‡†é‡æ’åº
                    print("â„¹ï¸  No location keywords in query, using standard reranking")
                    pairs = []
                    for doc in retrieved_docs:
                        doc_text = _format_document_for_rerank(doc)
                        pairs.append([query, doc_text])
                    
                    rerank_scores = models.reranker_model.predict(pairs, batch_size=1)
                    for doc, score in zip(retrieved_docs, rerank_scores):
                        doc["rerank_score"] = float(score)
                        doc["final_score"] = float(score)
                    
                    retrieved_docs = sorted(retrieved_docs, key=lambda x: x.get("final_score", 0), reverse=True)
                
                # æ›´æ–°æœ€ç»ˆæ’å
                for i, doc in enumerate(retrieved_docs):
                    doc['final_rank'] = i + 1
                
                rerank_time = time.time() - rerank_start
                print(f"ğŸ”„ Two-stage reranking completed in {rerank_time:.2f}s")
                
            except Exception as e:
                print(f"âš ï¸ Reranking failed: {e}, using vector scores only")
                import traceback
                traceback.print_exc()
                # é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ’å
                for i, doc in enumerate(retrieved_docs):
                    doc['final_rank'] = doc.get('rank', i + 1)
        else:
            # ä¸ä½¿ç”¨é‡æ’åºï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ’å
            for i, doc in enumerate(retrieved_docs):
                doc['final_rank'] = doc.get('rank', i + 1)
        
        # è°ƒè¯•ï¼šæ‰“å°ç¬¬ä¸€ä¸ªæ–‡æ¡£çš„å­—æ®µ
        if retrieved_docs:
            print(f"ğŸ“‹ First document fields: {list(retrieved_docs[0].keys())}")
            print(f"ğŸ“‹ Merchant name: {retrieved_docs[0].get('name', 'NOT FOUND')}")
        
        # 4. ç”Ÿæˆç­”æ¡ˆæ‘˜è¦ï¼ˆcity å·²ç»æ˜¯ä¸­æ–‡ï¼‰
        answer = f"åœ¨{city}æ‰¾åˆ° {len(retrieved_docs)} å®¶ç›¸å…³å•†æˆ·ï¼Œä¸ºæ‚¨æ¨èä»¥ä¸‹ {min(top_k, len(retrieved_docs))} å®¶ï¼š"
        
        # 5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "retrieved_count": len(retrieved_docs),
            "returned_count": min(top_k, len(retrieved_docs)),
            "city": city,
            "latency_ms": (time.time() - start_time) * 1000,
            "embedding_time_ms": embedding_time * 1000,
            "retrieval_time_ms": retrieval_time * 1000,
            "rerank_time_ms": rerank_time * 1000 if use_reranker else 0,
            "used_reranker": use_reranker,
            "candidate_multiplier": candidate_multiplier if use_reranker else 1
        }
        
        # è°ƒè¯•ï¼šæ‰“å°è¿”å›çš„å•†åº—åç§°
        top_merchants = retrieved_docs[:top_k]
        print(f"ğŸ“¦ Returning top {len(top_merchants)} merchants:")
        for i, doc in enumerate(top_merchants[:5], 1):  # æ‰“å°å‰5ä¸ª
            if use_reranker:
                location_info = f"loc={doc.get('location_score', 0):.2f}" if 'location_score' in doc else ""
                rerank_info = f"rerank={doc.get('rerank_score', 0):.4f}"
                final_info = f"final={doc.get('final_score', 0):.4f}"
                score_info = f"{location_info} {rerank_info} {final_info}".strip()
                geo_info = f"{doc.get('district', '?')}/{doc.get('business_area', '?')}"
            else:
                score_info = f"similarity={doc.get('similarity', 0):.4f}"
                geo_info = f"{doc.get('district', '?')}/{doc.get('business_area', '?')}"
            
            print(f"   {i}. {doc.get('name', 'NO_NAME')} | {geo_info} | {score_info} | rank: {doc.get('original_rank', '?')}â†’{doc.get('final_rank', '?')}")
        
        return {
            "answer": answer,
            "sources": retrieved_docs[:top_k],
            "metrics": metrics,
            "processing_time": time.time() - start_time
        }
        
    except Exception as e:
        print(f"âŒ RAG search error: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


def _extract_location_from_query(query: str) -> List[str]:
    """
    ä»æŸ¥è¯¢ä¸­æå–åœ°ç†ä½ç½®å…³é”®è¯
    
    æ”¯æŒçš„åœ°ç†å±‚çº§ï¼š
    - åŒºçº§ï¼šæµ¦ä¸œæ–°åŒºã€é»„æµ¦åŒºã€å¾æ±‡åŒºç­‰
    - å•†åœˆï¼šé™†å®¶å˜´ã€å—äº¬è·¯ã€æ·®æµ·è·¯ç­‰
    - åœ°æ ‡ï¼šä¸œæ–¹æ˜ç ã€äººæ°‘å¹¿åœºã€è™¹æ¡¥æœºåœºç­‰
    
    Returns:
        åœ°ç†ä½ç½®å…³é”®è¯åˆ—è¡¨
    """
    location_keywords = []
    
    # å¸¸è§åŒºåŸŸå…³é”®è¯
    district_patterns = ['åŒº', 'æ–°åŒº', 'å¿']
    for pattern in district_patterns:
        if pattern in query:
            # æå–"XXåŒº"ã€"XXæ–°åŒº"ç­‰
            import re
            matches = re.findall(r'[\u4e00-\u9fa5]+' + pattern, query)
            location_keywords.extend(matches)
    
    # å¸¸è§å•†åœˆ/åœ°æ ‡å…³é”®è¯ï¼ˆå¯æ‰©å±•ï¼‰
    common_areas = [
        'é™†å®¶å˜´', 'å—äº¬è·¯', 'æ·®æµ·è·¯', 'å¾å®¶æ±‡', 'äº”è§’åœº', 'ä¸­å±±å…¬å›­',
        'äººæ°‘å¹¿åœº', 'é™å®‰å¯º', 'è™¹æ¡¥', 'å¼ æ±Ÿ', 'é‡‘æ¡¥', 'ä¸–çºªå…¬å›­',
        'æ–°å¤©åœ°', 'ç”°å­åŠ', 'å¤–æ»©', 'è±«å›­', 'ä¸ƒå®', 'è˜åº„'
    ]
    
    for area in common_areas:
        if area in query:
            location_keywords.append(area)
    
    return location_keywords


def _calculate_location_relevance(doc_info: Dict[str, Any], location_keywords: List[str]) -> float:
    """
    è®¡ç®—æ–‡æ¡£ä¸åœ°ç†ä½ç½®çš„ç›¸å…³æ€§åˆ†æ•°
    
    åŒ¹é…ä¼˜å…ˆçº§ï¼š
    1. å•†åœˆ (business_area) - æƒé‡ 1.0
    2. åŒºåŸŸ (district) - æƒé‡ 0.8
    3. åœ°æ ‡ (landmark) - æƒé‡ 0.7
    4. åœ°å€ (address) - æƒé‡ 0.6
    
    Returns:
        åœ°ç†ç›¸å…³æ€§åˆ†æ•° (0-1)
    """
    if not location_keywords:
        return 1.0  # å¦‚æœæ²¡æœ‰åœ°ç†å…³é”®è¯ï¼Œä¸è¿‡æ»¤
    
    score = 0.0
    matched = False
    
    # æ£€æŸ¥å•†åœˆåŒ¹é…
    business_area = doc_info.get('business_area', '')
    for keyword in location_keywords:
        if keyword in business_area:
            score = max(score, 1.0)
            matched = True
            break
    
    # æ£€æŸ¥åŒºåŸŸåŒ¹é…
    district = doc_info.get('district', '')
    for keyword in location_keywords:
        if keyword in district:
            score = max(score, 0.8)
            matched = True
            break
    
    # æ£€æŸ¥åœ°æ ‡åŒ¹é…
    landmark = doc_info.get('landmark', '')
    for keyword in location_keywords:
        if keyword in landmark:
            score = max(score, 0.7)
            matched = True
            break
    
    # æ£€æŸ¥åœ°å€åŒ¹é…
    address = doc_info.get('address', '')
    for keyword in location_keywords:
        if keyword in address:
            score = max(score, 0.6)
            matched = True
            break
    
    return score if matched else 0.0


def _format_document_for_rerank(doc_info: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–æ–‡æ¡£ç”¨äºé‡æ’åºï¼ˆåœ°ç†ä¼˜å…ˆç­–ç•¥ï¼‰
    
    ğŸ”¥ æ–°ç­–ç•¥ï¼šå…ˆåœ°ç†ä½ç½®ï¼Œåå•†åº—ç±»å‹
    
    æ ¼å¼ç¤ºä¾‹ï¼š
        ä½ç½®ï¼šæµ¦ä¸œæ–°åŒºé™†å®¶å˜´å•†åœˆ ç±»å‹ï¼šé¤é¥®/å’–å•¡å… åº—åï¼šæ˜Ÿå·´å…‹å’–å•¡ ç‰¹è‰²ï¼šWiFi ç°ç£¨å’–å•¡
    
    Args:
        doc_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸
        
    Returns:
        æ ¼å¼åŒ–åçš„æ–‡æ¡£æ–‡æœ¬ï¼ˆåœ°ç†ä½ç½®å‰ç½®ï¼‰
    """
    parts = []
    
    # ğŸ”¥ 1. åœ°ç†ä½ç½®ï¼ˆæœ€ä¼˜å…ˆï¼‰
    location_parts = []
    if doc_info.get('district'):
        location_parts.append(doc_info['district'])
    if doc_info.get('business_area'):
        location_parts.append(doc_info['business_area'] + 'å•†åœˆ')
    if doc_info.get('landmark'):
        location_parts.append('è¿‘' + doc_info['landmark'])
    
    if location_parts:
        parts.append(f"ä½ç½®ï¼š{''.join(location_parts)}")
    
    # 2. ç±»å‹ï¼ˆç±»åˆ« + å­ç±»åˆ«ï¼‰
    category_parts = []
    if doc_info.get('category'):
        category_parts.append(doc_info['category'])
    if doc_info.get('subcategory'):
        category_parts.append(doc_info['subcategory'])
    
    if category_parts:
        parts.append(f"ç±»å‹ï¼š{'/'.join(category_parts)}")
    
    # 3. åº—å
    if doc_info.get('name'):
        parts.append(f"åº—åï¼š{doc_info['name']}")
    
    # 4. ç‰¹è‰²æœåŠ¡ï¼ˆé‡è¦ï¼‰
    if doc_info.get('specialties'):
        parts.append(f"ç‰¹è‰²ï¼š{doc_info['specialties']}")
    
    if doc_info.get('tags'):
        parts.append(f"æ ‡ç­¾ï¼š{doc_info['tags']}")
    
    # 5. å…¶ä»–ä¿¡æ¯
    if doc_info.get('products'):
        parts.append(f"æœåŠ¡ï¼š{doc_info['products']}")
    
    if doc_info.get('business_hours'):
        parts.append(f"è¥ä¸šï¼š{doc_info['business_hours']}")
    
    # ä½¿ç”¨å•ä¸ªç©ºæ ¼è¿æ¥æ‰€æœ‰éƒ¨åˆ†
    return ' '.join(parts)

def perform_web_search(query: str, top_k: int) -> Dict:
    """ä¼ ç»Ÿ Web æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Web æœç´¢é€»è¾‘
    # ä¾‹å¦‚ï¼šElasticSearch, BM25 ç­‰
    
    results = [
        {
            "merchant_name": f"å•†æˆ· {i+1}",
            "address": f"ä¸Šæµ·å¸‚æŸåŒºæŸè¡—é“{i+1}å·",
            "rating": 4.5 - i * 0.1,
            "price": f"äººå‡{100 + i*20}å…ƒ"
        }
        for i in range(top_k)
    ]
    
    return {
        "answer": f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ",
        "sources": results,
        "metrics": {"latency_ms": (time.time() - start_time) * 1000},
        "processing_time": time.time() - start_time
    }

def perform_agentic_search(query: str, model: str, max_iterations: int) -> Dict:
    """æ™ºèƒ½ä½“æœç´¢"""
    start_time = time.time()
    
    # å®ç°ä½ çš„ Agent é€»è¾‘
    # ä¾‹å¦‚ï¼šä½¿ç”¨ LangChain, AutoGPT ç­‰
    
    reasoning_steps = [
        "ğŸ¤” åˆ†ææŸ¥è¯¢æ„å›¾...",
        "ğŸ” ç¬¬1æ­¥ï¼šæœç´¢ç›¸å…³å•†æˆ·...",
        "ğŸ“Š ç¬¬2æ­¥ï¼šè¿‡æ»¤å’Œæ’åºç»“æœ...",
        "ğŸ’¡ ç¬¬3æ­¥ï¼šç”Ÿæˆæ¨è..."
    ]
    
    results = [
        {
            "merchant_name": "æ¨èå•†æˆ· 1",
            "address": "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒº",
            "rating": 4.8,
            "reason": "é«˜è¯„åˆ†ä¸”ç¬¦åˆæ‚¨çš„éœ€æ±‚"
        }
    ]
    
    return {
        "answer": "åŸºäºå¤šæ­¥æ¨ç†ï¼Œä¸ºæ‚¨æ¨è...",
        "sources": results,
        "metrics": {
            "correctness": 0.85,
            "completeness": 0.90,
            "faithfulness": 0.88,
            "latency_ms": (time.time() - start_time) * 1000
        },
        "reasoning_steps": reasoning_steps,
        "processing_time": time.time() - start_time
    }

# ==================== API ç«¯ç‚¹ ====================

@app.get("/")
def root():
    return {
        "service": "LocalSearchBench RAG API",
        "version": "1.0.0",
        "device": DEVICE,
        "status": "running",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
def health_check():
    """å¥åº·æ£€æŸ¥"""
    cities_loaded = {}
    if models and models.vector_db:
        # CityVectorDB ä½¿ç”¨ city_to_en æ˜ å°„ï¼ˆä¸­æ–‡ -> è‹±æ–‡ï¼‰
        # indexes å’Œ metadata çš„ key æ˜¯ä¸­æ–‡åŸå¸‚å
        for city_cn, city_en in models.vector_db.city_to_en.items():
            if city_cn in models.vector_db.indexes:
                cities_loaded[city_en] = {
                    "name": city_cn,
                    "vectors": models.vector_db.indexes[city_cn].ntotal,
                    "merchants": len(models.vector_db.metadata.get(city_cn, []))
                }
    
    return {
        "status": "healthy",
        "device": DEVICE,
        "gpu_available": torch.cuda.is_available() if 'torch' in globals() else False,
        "models_loaded": {
            "embedding": models.embedding_model is not None if models else False,
            "reranker": models.reranker_model is not None if models else False,
            "vector_db": models.vector_db is not None if models else False
        },
        "cities": cities_loaded,
        "total_cities": len(cities_loaded)
    }

@app.post("/api/rag/search", response_model=SearchResult)
async def rag_search(request: RAGSearchRequest):
    """RAG æœç´¢ç«¯ç‚¹ï¼ˆæ”¯æŒå¤šåŸå¸‚ï¼‰"""
    try:
        result = perform_rag_search(
            query=request.query,
            city=request.city,
            top_k=request.top_k,
            retriever=request.retriever,
            reranker=request.reranker
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/web/search", response_model=SearchResult)
async def web_search(request: WebSearchRequest):
    """Web æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_web_search(
            query=request.query,
            top_k=request.top_k
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/agentic/search", response_model=SearchResult)
async def agentic_search(request: AgenticSearchRequest):
    """Agentic æœç´¢ç«¯ç‚¹"""
    try:
        result = perform_agentic_search(
            query=request.query,
            model=request.model,
            max_iterations=request.max_iterations
        )
        return SearchResult(**result)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ¨¡å‹å’Œå‘é‡æ•°æ®åº“"""
    global models
    
    print("ğŸš€ Starting LocalSearchBench RAG Server...")
    print(f"ğŸ“ Device: {DEVICE}")
    
    # è·å–é…ç½®
    data_dir = getattr(app.state, 'data_dir', None)
    embedding_model_path = getattr(app.state, 'embedding_model_path', None)
    reranker_model_path = getattr(app.state, 'reranker_model_path', None)
    use_gpu = getattr(app.state, 'use_gpu', True)  # é»˜è®¤ä½¿ç”¨ GPU
    
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆåŒ…æ‹¬å‘é‡æ•°æ®åº“ï¼Œä¼šæ ¹æ® use_gpu å‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨ GPUï¼‰
    models = RAGModels(data_dir=data_dir, use_gpu=use_gpu)
    
    # é¢„åŠ è½½æ¨¡å‹åˆ° GPU
    if DEVICE == "cuda":
        print("\nğŸ“¥ Pre-loading models to GPU...")
        
        # åŠ è½½ Embedding æ¨¡å‹
        if embedding_model_path:
            models.load_embedding_model(embedding_model_path)
        else:
            print("âš ï¸  No embedding model path specified, using default")
            models.load_embedding_model()
        
        # åŠ è½½ Reranker æ¨¡å‹
        if reranker_model_path:
            models.load_reranker_model(reranker_model_path)
        else:
            print("âš ï¸  No reranker model path specified, using default")
            models.load_reranker_model()
        
        print("âœ… Models loaded successfully")
    else:
        print("âš ï¸ Running in CPU mode")
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“çŠ¶æ€
    if models.vector_db:
        print(f"\nâœ… Vector databases ready: {len(models.vector_db.indexes)} cities loaded")
    else:
        print("\nâš ï¸  No vector databases loaded. Please specify --data-dir")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­æ—¶æ¸…ç†èµ„æº"""
    print("ğŸ‘‹ Shutting down LocalSearchBench RAG Server...")
    # æ¸…ç† GPU æ˜¾å­˜
    if DEVICE == "cuda" and 'torch' in globals():
        torch.cuda.empty_cache()

# ==================== ä¸»å‡½æ•° ====================

def main():
    parser = argparse.ArgumentParser(description="LocalSearchBench RAG Server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind")
    parser.add_argument("--data-dir", type=str, default=None, help="Path to vector database directory (containing 1028 FAISS files)")
    parser.add_argument("--embedding-model", type=str, default=None, help="Path to Qwen3-Embedding-8B model")
    parser.add_argument("--reranker-model", type=str, default=None, help="Path to Qwen3-Reranker-8B model")
    parser.add_argument("--use-gpu", action="store_true", default=True, help="Use GPU for FAISS vector search (default: True)")
    parser.add_argument("--no-gpu", action="store_true", help="Force CPU mode for FAISS vector search")
    parser.add_argument("--reload", action="store_true", help="Enable auto-reload")
    parser.add_argument("--workers", type=int, default=1, help="Number of workers")
    
    args = parser.parse_args()
    
    # ä»ç¯å¢ƒå˜é‡æˆ–å‘½ä»¤è¡Œå‚æ•°è·å–é…ç½®
    data_dir = args.data_dir or os.getenv("RAG_DATA_DIR")
    embedding_model_path = args.embedding_model or os.getenv("EMBEDDING_MODEL_PATH")
    reranker_model_path = args.reranker_model or os.getenv("RERANKER_MODEL_PATH")
    
    # GPU é…ç½®
    use_gpu = args.use_gpu and not args.no_gpu
    
    # å°†é…ç½®ä¿å­˜åˆ°å…¨å±€å˜é‡ä¾› startup_event ä½¿ç”¨
    app.state.data_dir = data_dir
    app.state.embedding_model_path = embedding_model_path
    app.state.reranker_model_path = reranker_model_path
    app.state.use_gpu = use_gpu
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     LocalSearchBench RAG Server (Multi-City Support)      â•‘
â•‘     Device: {DEVICE:48s} â•‘
â•‘     Host: {args.host:50s} â•‘
â•‘     Port: {args.port:50d} â•‘
â•‘     Data Dir: {(data_dir or 'Not specified')[:45]:45s} â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    uvicorn.run(
        app,  # ç›´æ¥ä¼ å…¥ app å¯¹è±¡ï¼Œè€Œä¸æ˜¯å­—ç¬¦ä¸²
        host=args.host,
        port=args.port,
        reload=args.reload,
        workers=args.workers,
        log_level="info"
    )

if __name__ == "__main__":
    main()

